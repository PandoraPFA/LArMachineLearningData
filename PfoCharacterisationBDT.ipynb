{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the relevant scripts from LArMachineLearningData\n",
    "# Nice the process so it can run with lots of cores on low priority\n",
    "import os\n",
    "os.nice(20)\n",
    "\n",
    "# Add path for LArMachineLearningData\n",
    "import sys\n",
    "pandoraMVADir = os.environ['MY_TEST_AREA'] + '/LArMachineLearningData/'\n",
    "sys.path.append(pandoraMVADir + 'scripts')\n",
    "\n",
    "from PandoraBDT import *\n",
    "\n",
    "# Import relevant SKLearn stuff\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# Set global params\n",
    "testTrainFraction = 0.5\n",
    "nCores = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some analysis specific things\n",
    "trainingFile = pandoraMVADir + 'training_output.txt'\n",
    "\n",
    "BDTName = \"PFOCharBDT\"\n",
    "\n",
    "featureNames = ['Length', \n",
    "                'Straight Line Diff Mean',\n",
    "                'Max Fit Gap Length', \n",
    "                'Sliding Linear Fit RMS',\n",
    "                'Vertex Distance', \n",
    "                'PCA Secondary-Primary EigenValue Ratio',\n",
    "                'PCA Tertiary-Primary EigenValue Ratio',\n",
    "                'Opening Angle Diff',\n",
    "                'Fractional Spread',\n",
    "                'End Fraction'\n",
    "               ]\n",
    "\n",
    "\n",
    "# Set background and signal label names\n",
    "params = {\n",
    "    'labelNames': ['True Shower','True Track'],\n",
    "    'signalDefs': [0, 1],\n",
    "    'signalCols': ['r', 'b'],\n",
    "    'nBins': 100,\n",
    "    'PlotStep': 1.0,\n",
    "    'OptimalBinCut': 50,\n",
    "    'OptimalScoreCut': 0.5,\n",
    "    'nTrees': 100,\n",
    "    'TreeDepth': 3,\n",
    "    'logY': False\n",
    "}\n",
    "\n",
    "# Create the base BDT to vary the params from and compare to\n",
    "baseBDT = AdaBoostClassifier(DecisionTreeClassifier(max_depth=params['TreeDepth']),algorithm='SAMME', \n",
    "                         random_state=42, n_estimators=params['nTrees'])\n",
    "\n",
    "# Split the data into many subsets to grid search over (Set seed for reproducibility)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data, nFeatures, nExamples = LoadData(trainingFile, ',')\n",
    "featuresOrg, labelsOrg = SplitTrainingSet(data, nFeatures)\n",
    "features, labels = Randomize(featuresOrg, labelsOrg, True)\n",
    "\n",
    "# Split into train and test samples\n",
    "xTrain, yTrain, xTest, yTest = Sample(features, labels, testTrainFraction)\n",
    "\n",
    "# Split into signal and background based on the true labels\n",
    "signalFeatures = features[labels==1]\n",
    "backgroundFeatures = features[labels==0]\n",
    "\n",
    "# Check the features array is the same size as the feature names array\n",
    "print (len(featureNames))\n",
    "print (np.shape(features))\n",
    "print('Total: '+str(len(features))+', signal: '+\n",
    "      str(len(signalFeatures))+' and background: '+\n",
    "      str(len(backgroundFeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Pandas dataframe\n",
    "# First crete a dictionary\n",
    "allDict = {featureNames[i]: features[:, i] for i in range(nFeatures)}\n",
    "allDict.update({'Labels': labels})\n",
    "\n",
    "# Create the Pandas dataframe, create seperate df for signal/background\n",
    "df = pd.DataFrame(data=allDict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make plots drawing the variables for signal/background\n",
    "DrawVariablesDF(df, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make correlation matricies\n",
    "dfSig = df[df['Labels']==params['signalDefs'][0]].drop('Labels', axis=1)\n",
    "dfBck = df[df['Labels']==params['signalDefs'][1]].drop('Labels', axis=1)\n",
    "\n",
    "CorrelationDF(dfSig, params['labelNames'][0] + ' Correlation Matrix')\n",
    "CorrelationDF(dfBck, params['labelNames'][1] + ' Correlation Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to make a plot comparing two variables;\n",
    "xMetric = 'Vertex Distance'\n",
    "yMetric = 'PCA Secondary-Primary EigenValue Ratio'\n",
    "\n",
    "sns.jointplot(data=df, x=xMetric, y=yMetric, hue='Labels',\n",
    "              xlim=(np.quantile(df[xMetric], 0.02), np.quantile(df[xMetric], 0.98)), \n",
    "              ylim=(np.quantile(df[yMetric], 0.02), np.quantile(df[yMetric], 0.98)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting all combos, not very useful when we have too many variables\n",
    "sns.pairplot(df, hue='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of grid search\n",
    "depthRange = 3\n",
    "treeRange = 3\n",
    "\n",
    "# Set up ranges for grid search\n",
    "depthArray = np.linspace(1, depthRange, depthRange, dtype=int)\n",
    "treeArray = np.logspace(0, treeRange-1, treeRange, dtype=int)\n",
    "#treeArray = np.linspace(100, 100*treeRange, treeRange, dtype=int)\n",
    "\n",
    "# Print arrays for debugging\n",
    "print (\"Depth Array:\", depthArray)\n",
    "print (\"Tree Array: \", treeArray)\n",
    "\n",
    "# Construct a dictionary to loop over\n",
    "paramGrid = dict(base_estimator__max_depth=depthArray, n_estimators=treeArray)\n",
    "\n",
    "# Perform the grid search\n",
    "grid = GridSearchCV(baseBDT, param_grid=paramGrid, cv=cv, n_jobs=nCores, \n",
    "                    verbose=9, refit=True, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search\n",
    "grid.fit(xTrain, yTrain)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% \n",
    "      (grid.best_params_, grid.best_score_))\n",
    "\n",
    "# Put the output of the grid in a conveneant df\n",
    "gridResults = pd.DataFrame(grid.cv_results_)\n",
    "gridResults.rename(columns={\"param_base_estimator__max_depth\": \"MaxDepth\"}, inplace=True)\n",
    "gridResults.rename(columns={\"param_n_estimators\": \"NTrees\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testScores = gridResults.pivot(\"MaxDepth\", \"NTrees\", \"mean_test_score\")\n",
    "testStd = gridResults.pivot(\"MaxDepth\", \"NTrees\", \"std_test_score\")\n",
    "trainScores = gridResults.pivot(\"MaxDepth\", \"NTrees\", \"mean_train_score\")\n",
    "\n",
    "trainTestDiff = trainScores - testScores\n",
    "\n",
    "plt.figure(figsize=(4, 4), constrained_layout=True)\n",
    "sns.heatmap(testScores, cmap='bwr', linewidths=0, annot=True)\n",
    "plt.title('Validation accuracy: Test')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig('TestScores.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4), constrained_layout=True)\n",
    "sns.heatmap(testStd, cmap='bwr', linewidths=0, annot=True)\n",
    "plt.title('Validation accuracy: Std Test Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig('TrainStds.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4), constrained_layout=True)\n",
    "sns.heatmap(trainScores, cmap='bwr', linewidths=0, annot=True)\n",
    "plt.title('Validation accuracy: Train')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig('TrainScores.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4), constrained_layout=True)\n",
    "sns.heatmap(trainTestDiff, cmap='bwr', linewidths=0, annot=True)\n",
    "plt.title('Validation accuracy: Train Test Diff')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig('TrainTestDiff.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference BDT with controlled hyperparams\n",
    "baseBDT.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots()\n",
    "metrics.plot_roc_curve(grid, xTest, yTest, ax=ax)\n",
    "metrics.plot_roc_curve(baseBDT, xTest, yTest, ax=ax)\n",
    "\n",
    "plt.title(\"ROC Curves\")\n",
    "ax.invert_xaxis()\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matricies\n",
    "fig, ax = plt.subplots()\n",
    "metrics.plot_confusion_matrix(grid, xTest, yTest, display_labels=params['labelNames'],\n",
    "                             ax=ax, normalize='true')\n",
    "ax.invert_xaxis()\n",
    "#ax.invert_zaxis()\n",
    "plt.title(\"Confusion matrix (True Normalised)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print more detailed performance info\n",
    "bdtPredicted = baseBDT.predict(xTest)\n",
    "gridPredicted = grid.predict(xTest)\n",
    "\n",
    "print (\"Background (0): \", params['labelNames'][0])\n",
    "print (\"Signal (1): \", params['labelNames'][1])\n",
    "print (\"BDT:\\n\", metrics.classification_report(yTest, bdtPredicted))\n",
    "print (\"Grid:\\n\", metrics.classification_report(yTest, gridPredicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search performance over training sample size\n",
    "train_sizes_array = np.linspace(0.0,1, 20)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(baseBDT, features,\n",
    "    labels, train_sizes=train_sizes_array[1:], n_jobs=nCores, verbose=9, cv=cv)\n",
    "\n",
    "mean_train_scores = np.mean(train_scores, axis=1)\n",
    "mean_test_scores = np.mean(test_scores, axis=1)\n",
    "\n",
    "std_train_scores = np.std(train_scores, axis=1)\n",
    "std_test_scores = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progression\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Training Progression\")\n",
    "plt.xlabel(\"Number of Training Examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.plot(train_sizes, mean_train_scores, label='Train Score', color='b')\n",
    "plt.fill_between(train_sizes, mean_train_scores - std_train_scores,\n",
    "                         mean_train_scores + std_train_scores, alpha=0.1,\n",
    "                         color=\"b\")\n",
    "\n",
    "plt.plot(train_sizes, mean_test_scores, label='Test Score', color='r')\n",
    "plt.fill_between(train_sizes, mean_test_scores - std_test_scores,\n",
    "                         mean_test_scores + std_test_scores, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "#plt.plot(train_sizes, std_test_scores, label='Test Score Std.', color='k')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over a metric\n",
    "cppalplhaArray = np.linspace(0,0.001,11)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    baseBDT, features, labels, param_name='base_estimator__ccp_alpha',\n",
    "    param_range=cppalplhaArray, n_jobs=nCores, verbose=9, cv=cv)\n",
    "\n",
    "mean_train_scores = np.mean(train_scores, axis=1)\n",
    "mean_test_scores = np.mean(test_scores, axis=1)\n",
    "\n",
    "std_train_scores = np.std(train_scores, axis=1)\n",
    "std_test_scores = np.std(test_scores, axis=1)\n",
    "\n",
    "print (\"Means: \"+str(mean_test_scores)+\" and std. \"\n",
    "       +str(std_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid search\n",
    "plt.plot(cppalplhaArray, mean_train_scores, label='Train Score', color='b')\n",
    "plt.fill_between(cppalplhaArray, mean_train_scores - std_train_scores,\n",
    "                         mean_train_scores + std_train_scores, alpha=0.1,\n",
    "                         color=\"b\")\n",
    "plt.plot(cppalplhaArray, mean_test_scores, label='Test Score', color='r')\n",
    "plt.fill_between(cppalplhaArray, mean_test_scores - std_test_scores,\n",
    "                         mean_test_scores + std_test_scores, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "plt.grid()\n",
    "#plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over a metric\n",
    "learningRateArray = np.linspace(0.1,1.5,15)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    baseBDT, features, labels, param_name='learning_rate',\n",
    "    param_range=learningRateArray, n_jobs=nCores, verbose=9, cv=cv)\n",
    "\n",
    "mean_train_scores = np.mean(train_scores, axis=1)\n",
    "mean_test_scores = np.mean(test_scores, axis=1)\n",
    "\n",
    "std_train_scores = np.std(train_scores, axis=1)\n",
    "std_test_scores = np.std(test_scores, axis=1)\n",
    "\n",
    "print (\"Means: \"+str(mean_test_scores)+\" and std. \"\n",
    "       +str(std_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid search\n",
    "plt.plot(learningRateArray, mean_train_scores, label='Train Score', color='b')\n",
    "plt.fill_between(learningRateArray, mean_train_scores - std_train_scores,\n",
    "                         mean_train_scores + std_train_scores, alpha=0.1,\n",
    "                         color=\"b\")\n",
    "plt.plot(learningRateArray, mean_test_scores, label='Test Score', color='r')\n",
    "plt.fill_between(learningRateArray, mean_test_scores - std_test_scores,\n",
    "                         mean_test_scores + std_test_scores, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "plt.grid()\n",
    "#plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importance of features\n",
    "importanceDF = pd.DataFrame({'Features': featureNames, 'Importance Score':baseBDT.feature_importances_})\n",
    "print (importanceDF.sort_values(by=['Importance Score']))\n",
    "ax = importanceDF.sort_values(by=['Importance Score'])\\\n",
    "    .plot(kind='barh', x='Features', y='Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all tunable params\n",
    "baseBDT.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PandoraBDT\n",
    "from importlib import reload\n",
    "\n",
    "reload (PandoraBDT)\n",
    "from PandoraBDT import *\n",
    "\n",
    "print (np.shape(xTest))\n",
    "print (np.shape(yTest))\n",
    "print (np.shape(xTrain))\n",
    "print (np.shape(yTrain))\n",
    "\n",
    "\n",
    "PlotBdtKSScores(baseBDT, xTest, yTest, xTrain, yTrain, 'Vertex Region', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WriteXmlFile(BDTName+\".xml\", baseBDT, BDTName)\n",
    "SerializeToPkl(BDTName+\".pkl\", baseBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
