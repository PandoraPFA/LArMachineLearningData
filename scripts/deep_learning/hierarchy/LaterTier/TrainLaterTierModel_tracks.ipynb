{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1752bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Imports\n",
    "###########################################################\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.getcwd()[0:len(os.getcwd()) - 10])\n",
    "sys.path.insert(1, os.getcwd()[0:len(os.getcwd()) - 10] + '/Metrics')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "\n",
    "import Models\n",
    "import Datasets\n",
    "import TrainingMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Define file\n",
    "###########################################################\n",
    "\n",
    "trainFileName = sys.path[0] + '/files/hierarchy_TRAIN_later_tier_track.npz'\n",
    "\n",
    "branchModelPath = sys.path[0] + '/models/track_track_branch_model'\n",
    "classifierModelPath = sys.path[0] + '/models/track_track_classifier_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bac12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Hyperparameters\n",
    "###########################################################\n",
    "\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Get data from file\n",
    "###########################################################\n",
    "\n",
    "data = np.load(trainFileName)\n",
    "\n",
    "# Variables\n",
    "variables_train = data['variables_train']\n",
    "variables_test = data['variables_test']\n",
    "\n",
    "# Truth\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "trueParentChildLink_train = data['trueParentChildLink_train']\n",
    "trueParentChildLink_test = data['trueParentChildLink_test']\n",
    "trueChildVisibleGeneration_train = data['trueChildVisibleGeneration_train']\n",
    "trueChildVisibleGeneration_test = data['trueChildVisibleGeneration_test']\n",
    "trainingCutSep_train = data['trainingCutSep_train']\n",
    "trainingCutSep_test = data['trainingCutSep_test']\n",
    "trainingCutDoesConnect_train = data['trainingCutDoesConnect_train']\n",
    "trainingCutDoesConnect_test = data['trainingCutDoesConnect_test']\n",
    "trainingCutL_train = data['trainingCutL_train']\n",
    "trainingCutL_test = data['trainingCutL_test']\n",
    "trainingCutT_train = data['trainingCutT_train']\n",
    "trainingCutT_test = data['trainingCutT_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30449dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Check shapes\n",
    "###########################################################\n",
    "\n",
    "print('variables_train.shape:', variables_train.shape)\n",
    "print('variables_test.shape:', variables_test.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('y_test.shape:', y_test.shape)\n",
    "print('trueParentChildLink_train.shape:', trueParentChildLink_train.shape)\n",
    "print('trueParentChildLink_test.shape:', trueParentChildLink_test.shape)\n",
    "print('trueChildVisibleGeneration_train.shape:', trueChildVisibleGeneration_train.shape)\n",
    "print('trueChildVisibleGeneration_test.shape:', trueChildVisibleGeneration_test.shape)\n",
    "print('trainingCutSep_train.shape:', trainingCutSep_train.shape)\n",
    "print('trainingCutSep_test.shape:', trainingCutSep_test.shape)\n",
    "print('trainingCutDoesConnect_train.shape:', trainingCutDoesConnect_train.shape)\n",
    "print('trainingCutDoesConnect_test.shape:', trainingCutDoesConnect_test.shape)\n",
    "print('trainingCutL_train.shape:', trainingCutL_train.shape)\n",
    "print('trainingCutL_test.shape:', trainingCutL_test.shape)\n",
    "print('trainingCutT_train.shape:', trainingCutT_train.shape)\n",
    "print('trainingCutT_test.shape:', trainingCutT_test.shape)\n",
    "\n",
    "nVariables = variables_train.shape[1]\n",
    "nLinks = y_train.shape[1]\n",
    "ntrain = variables_train.shape[0]\n",
    "ntest  = variables_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Apply training cut topology mask\n",
    "###########################################################\n",
    "\n",
    "# These are inspired by hand-scanning :'(\n",
    "MAX_TRAINING_CUT_SEP = 20.0\n",
    "MIN_TRAINING_CUT_L = -100.0\n",
    "MAX_TRAINING_CUT_L = 100.0\n",
    "MAX_TRAINING_CUT_T = 40.0\n",
    "\n",
    "######################\n",
    "# training set first\n",
    "######################\n",
    "# Make mask\n",
    "passTrainingCutSep_train = trainingCutSep_train < MAX_TRAINING_CUT_SEP\n",
    "passTrainingCutDoesConnect_train = trainingCutDoesConnect_train == 1\n",
    "passTrainingCutL_train = np.logical_and(trainingCutL_train > MIN_TRAINING_CUT_L, trainingCutL_train < MAX_TRAINING_CUT_L)\n",
    "passTrainingCutT_train = trainingCutT_train < MAX_TRAINING_CUT_T\n",
    "# passTrainingCuts_train = np.logical_or(passTrainingCutSep_train, np.logical_and(passTrainingCutL_train, passTrainingCutT_train))\n",
    "passTrainingCuts_train = np.logical_or(passTrainingCutSep_train, np.logical_or(passTrainingCutDoesConnect_train, np.logical_and(passTrainingCutL_train, passTrainingCutT_train)))\n",
    "\n",
    "# Mask the 1D variables... shape=(nEntries, )\n",
    "trueChildVisibleGeneration_train = trueChildVisibleGeneration_train[passTrainingCuts_train]\n",
    "trueParentChildLink_train = trueParentChildLink_train[passTrainingCuts_train]\n",
    "trainingCutSep_train = trainingCutSep_train[passTrainingCuts_train] # Don't really need to do this as we're not using them again\n",
    "trainingCutDoesConnect_train = trainingCutDoesConnect_train[passTrainingCuts_train] # Don't really need to do this as we're not using them again\n",
    "trainingCutL_train = trainingCutL_train[passTrainingCuts_train]     # Don't really need to do this as we're not using them again\n",
    "trainingCutT_train = trainingCutT_train[passTrainingCuts_train]     # Don't really need to do this as we're not using them again\n",
    "\n",
    "# Mask the truth... shape=(nEntries, nLinks)\n",
    "y_train = y_train[np.column_stack((passTrainingCuts_train, passTrainingCuts_train, passTrainingCuts_train, passTrainingCuts_train))].reshape(-1, nLinks)\n",
    "\n",
    "# Mask the variable... shape=(nEntries, nVariables)\n",
    "variables_train = variables_train[[[entry] * nVariables for entry in passTrainingCuts_train]].reshape(-1, nVariables)\n",
    "\n",
    "######################\n",
    "# now test set\n",
    "######################\n",
    "# Make mask\n",
    "passTrainingCutSep_test = trainingCutSep_test < MAX_TRAINING_CUT_SEP\n",
    "passTrainingCutDoesConnect_test = trainingCutDoesConnect_test == 1\n",
    "passTrainingCutL_test = np.logical_and(trainingCutL_test > MIN_TRAINING_CUT_L, trainingCutL_test < MAX_TRAINING_CUT_L)\n",
    "passTrainingCutT_test = trainingCutT_test < MAX_TRAINING_CUT_T\n",
    "# passTrainingCuts_test = np.logical_or(passTrainingCutSep_test, np.logical_and(passTrainingCutL_test, passTrainingCutT_test))\n",
    "passTrainingCuts_test = np.logical_or(passTrainingCutSep_test, np.logical_or(passTrainingCutDoesConnect_test, np.logical_and(passTrainingCutL_test, passTrainingCutT_test)))\n",
    "\n",
    "# Mask the 1D variables... shape=(nEntries, )\n",
    "trueChildVisibleGeneration_test = trueChildVisibleGeneration_test[passTrainingCuts_test]\n",
    "trueParentChildLink_test = trueParentChildLink_test[passTrainingCuts_test]\n",
    "trainingCutSep_test = trainingCutSep_test[passTrainingCuts_test] # Don't really need to do this as we're not using them again\n",
    "trainingCutDoesConnect_test = trainingCutDoesConnect_test[passTrainingCuts_test] # Don't really need to do this as we're not using them again\n",
    "trainingCutL_test = trainingCutL_test[passTrainingCuts_test]     # Don't really need to do this as we're not using them again\n",
    "trainingCutT_test = trainingCutT_test[passTrainingCuts_test]     # Don't really need to do this as we're not using them again\n",
    "\n",
    "# Mask the truth... shape=(nEntries, nLinks)\n",
    "y_test = y_test[np.column_stack((passTrainingCuts_test, passTrainingCuts_test, passTrainingCuts_test, passTrainingCuts_test))].reshape(-1, nLinks)\n",
    "\n",
    "# Mask the variable... shape=(nEntries, nVariables)\n",
    "variables_test = variables_test[[[entry] * nVariables for entry in passTrainingCuts_test]].reshape(-1, nVariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Check shapes\n",
    "###########################################################\n",
    "\n",
    "print('variables_train.shape:', variables_train.shape)\n",
    "print('variables_test.shape:', variables_test.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('y_test.shape:', y_test.shape)\n",
    "print('trueParentChildLink_train.shape:', trueParentChildLink_train.shape)\n",
    "print('trueParentChildLink_test.shape:', trueParentChildLink_test.shape)\n",
    "print('trueChildVisibleGeneration_train.shape:', trueChildVisibleGeneration_train.shape)\n",
    "print('trueChildVisibleGeneration_test.shape:', trueChildVisibleGeneration_test.shape)\n",
    "print('trainingCutSep_train.shape:', trainingCutSep_train.shape)\n",
    "print('trainingCutSep_test.shape:', trainingCutSep_test.shape)\n",
    "print('trainingCutDoesConnect_train.shape:', trainingCutDoesConnect_train.shape)\n",
    "print('trainingCutDoesConnect_test.shape:', trainingCutDoesConnect_test.shape)\n",
    "print('trainingCutL_train.shape:', trainingCutL_train.shape)\n",
    "print('trainingCutL_test.shape:', trainingCutL_test.shape)\n",
    "print('trainingCutT_train.shape:', trainingCutT_train.shape)\n",
    "print('trainingCutT_test.shape:', trainingCutT_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ec1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Branch weights!\n",
    "###########################################################\n",
    "\n",
    "# Calculate branch weights\n",
    "nTrue = np.count_nonzero(y_train == 1)\n",
    "nBackground = np.count_nonzero(y_train == 0)\n",
    "nWrongOrientation = np.count_nonzero(y_train == 2)\n",
    "\n",
    "maxLinks = max(nTrue, nBackground, nWrongOrientation)\n",
    "\n",
    "true_branch_weight = float(maxLinks)/float(nTrue)\n",
    "background_branch_weight = float(maxLinks)/float(nBackground)\n",
    "wrong_orientation_branch_weight = float(maxLinks)/float(nWrongOrientation)\n",
    "\n",
    "print(nTrue)\n",
    "print(background_branch_weight)\n",
    "print(wrong_orientation_branch_weight)\n",
    "\n",
    "# Combine for branch!\n",
    "n_secondary_child_true_branch = np.count_nonzero(np.logical_and(y_train[:,0] == 1, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,1] == 1, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,2] == 1, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,3] == 1, (trueChildVisibleGeneration_train == 3)))\n",
    "n_secondary_child_background_branch = np.count_nonzero(np.logical_and(y_train[:,0] == 0, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,1] == 0, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,2] == 0, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,3] == 0, (trueChildVisibleGeneration_train == 3)))\n",
    "n_secondary_child_wrong_orientation_branch = np.count_nonzero(np.logical_and(y_train[:,0] == 2, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,1] == 2, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,2] == 2, (trueChildVisibleGeneration_train == 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,3] == 2, (trueChildVisibleGeneration_train == 3))) \n",
    "\n",
    "n_higher_child_true_branch = np.count_nonzero(np.logical_and(y_train[:,0] == 1, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,1] == 1, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,2] == 1, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,3] == 1, (trueChildVisibleGeneration_train > 3)))\n",
    "n_higher_child_background_branch = np.count_nonzero(np.logical_and(y_train[:,0] == 0, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,1] == 0, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,2] == 0, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,3] == 0, (trueChildVisibleGeneration_train > 3)))\n",
    "n_higher_child_wrong_orientation_branch = np.count_nonzero(np.logical_and(y_train[:,0] == 2, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,1] == 2, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,2] == 2, (trueChildVisibleGeneration_train > 3))) + \\\n",
    "    np.count_nonzero(np.logical_and(y_train[:,3] == 2, (trueChildVisibleGeneration_train > 3)))\n",
    "\n",
    "secondary_child_true_branch_weight = (nTrue * 0.5) / n_secondary_child_true_branch\n",
    "secondary_child_background_branch_weight = (nBackground * 0.5) / n_secondary_child_background_branch\n",
    "secondary_child_wrong_orientation_branch_weight = (nWrongOrientation * 0.5) / n_secondary_child_wrong_orientation_branch\n",
    "\n",
    "higher_child_true_branch_weight = (nTrue * 0.5) / n_higher_child_true_branch\n",
    "higher_child_background_branch_weight = (nBackground * 0.5) / n_higher_child_background_branch\n",
    "higher_child_wrong_orientation_branch_weight = (nWrongOrientation * 0.5) / n_higher_child_wrong_orientation_branch\n",
    "\n",
    "classWeights_branch = {\n",
    "    'secondary_child_true_branch_weight'              : (secondary_child_true_branch_weight * true_branch_weight), \n",
    "    'secondary_child_background_branch_weight'        : (secondary_child_background_branch_weight * background_branch_weight),\n",
    "    'secondary_child_wrong_orientation_branch_weight' : (secondary_child_wrong_orientation_branch_weight * wrong_orientation_branch_weight),    \n",
    "    'higher_child_true_branch_weight'                 : (higher_child_true_branch_weight * true_branch_weight),\n",
    "    'higher_child_background_branch_weight'           : (higher_child_background_branch_weight * background_branch_weight),\n",
    "    'higher_child_wrong_orientation_branch_weight'    : (higher_child_wrong_orientation_branch_weight * wrong_orientation_branch_weight)\n",
    "}\n",
    "\n",
    "print('classWeights_branch:', classWeights_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK!\n",
    "\n",
    "this_signal = ((np.count_nonzero(np.logical_and(y_train[:,0] == 1, (trueChildVisibleGeneration_train == 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,1] == 1, (trueChildVisibleGeneration_train == 3))) +\n",
    "                np.count_nonzero(np.logical_and(y_train[:,2] == 1, (trueChildVisibleGeneration_train == 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,3] == 1, (trueChildVisibleGeneration_train == 3)))) * classWeights_branch['secondary_child_true_branch_weight']) + \\\n",
    "              ((np.count_nonzero(np.logical_and(y_train[:,0] == 1, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,1] == 1, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,2] == 1, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,3] == 1, (trueChildVisibleGeneration_train > 3)))) * classWeights_branch['higher_child_true_branch_weight'])\n",
    "\n",
    "this_background = ((np.count_nonzero(np.logical_and(y_train[:,0] == 0, (trueChildVisibleGeneration_train == 3))) + \n",
    "                    np.count_nonzero(np.logical_and(y_train[:,1] == 0, (trueChildVisibleGeneration_train == 3))) + \n",
    "                    np.count_nonzero(np.logical_and(y_train[:,2] == 0, (trueChildVisibleGeneration_train == 3))) + \n",
    "                    np.count_nonzero(np.logical_and(y_train[:,3] == 0, (trueChildVisibleGeneration_train == 3)))) * classWeights_branch['secondary_child_background_branch_weight']) + \\\n",
    "                  ((np.count_nonzero(np.logical_and(y_train[:,0] == 0, (trueChildVisibleGeneration_train > 3))) + \n",
    "                    np.count_nonzero(np.logical_and(y_train[:,1] == 0, (trueChildVisibleGeneration_train > 3))) +\n",
    "                    np.count_nonzero(np.logical_and(y_train[:,2] == 0, (trueChildVisibleGeneration_train > 3))) +\n",
    "                    np.count_nonzero(np.logical_and(y_train[:,3] == 0, (trueChildVisibleGeneration_train > 3)))) * classWeights_branch['higher_child_background_branch_weight'])\n",
    "\n",
    "this_wrong_orientation = ((np.count_nonzero(np.logical_and(y_train[:,0] == 2, (trueChildVisibleGeneration_train == 3))) + \n",
    "                           np.count_nonzero(np.logical_and(y_train[:,1] == 2, (trueChildVisibleGeneration_train == 3))) + \n",
    "                           np.count_nonzero(np.logical_and(y_train[:,2] == 2, (trueChildVisibleGeneration_train == 3))) + \n",
    "                           np.count_nonzero(np.logical_and(y_train[:,3] == 2, (trueChildVisibleGeneration_train == 3)))) * classWeights_branch['secondary_child_wrong_orientation_branch_weight']) + \\\n",
    "                         ((np.count_nonzero(np.logical_and(y_train[:,0] == 2, (trueChildVisibleGeneration_train > 3))) + \n",
    "                           np.count_nonzero(np.logical_and(y_train[:,1] == 2, (trueChildVisibleGeneration_train > 3))) +\n",
    "                           np.count_nonzero(np.logical_and(y_train[:,2] == 2, (trueChildVisibleGeneration_train > 3))) +\n",
    "                           np.count_nonzero(np.logical_and(y_train[:,3] == 2, (trueChildVisibleGeneration_train > 3)))) * classWeights_branch['higher_child_wrong_orientation_branch_weight'])\n",
    "\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "this_secondary = ((np.count_nonzero(np.logical_and(y_train[:,0] == 1, (trueChildVisibleGeneration_train == 3))) + \n",
    "                   np.count_nonzero(np.logical_and(y_train[:,1] == 1, (trueChildVisibleGeneration_train == 3))) +\n",
    "                   np.count_nonzero(np.logical_and(y_train[:,2] == 1, (trueChildVisibleGeneration_train == 3))) + \n",
    "                   np.count_nonzero(np.logical_and(y_train[:,3] == 1, (trueChildVisibleGeneration_train == 3)))) * classWeights_branch['secondary_child_true_branch_weight']) + \\\n",
    "                 ((np.count_nonzero(np.logical_and(y_train[:,0] == 0, (trueChildVisibleGeneration_train == 3))) + \n",
    "                   np.count_nonzero(np.logical_and(y_train[:,1] == 0, (trueChildVisibleGeneration_train == 3))) + \n",
    "                   np.count_nonzero(np.logical_and(y_train[:,2] == 0, (trueChildVisibleGeneration_train == 3))) + \n",
    "                   np.count_nonzero(np.logical_and(y_train[:,3] == 0, (trueChildVisibleGeneration_train == 3)))) * classWeights_branch['secondary_child_background_branch_weight']) + \\\n",
    "                 ((np.count_nonzero(np.logical_and(y_train[:,0] == 2, (trueChildVisibleGeneration_train == 3))) + \n",
    "                   np.count_nonzero(np.logical_and(y_train[:,1] == 2, (trueChildVisibleGeneration_train == 3))) + \n",
    "                   np.count_nonzero(np.logical_and(y_train[:,2] == 2, (trueChildVisibleGeneration_train == 3))) + \n",
    "                   np.count_nonzero(np.logical_and(y_train[:,3] == 2, (trueChildVisibleGeneration_train == 3)))) * classWeights_branch['secondary_child_wrong_orientation_branch_weight'])\n",
    "\n",
    "this_higher = ((np.count_nonzero(np.logical_and(y_train[:,0] == 1, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,1] == 1, (trueChildVisibleGeneration_train > 3))) +\n",
    "                np.count_nonzero(np.logical_and(y_train[:,2] == 1, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,3] == 1, (trueChildVisibleGeneration_train > 3)))) * classWeights_branch['higher_child_true_branch_weight']) + \\\n",
    "              ((np.count_nonzero(np.logical_and(y_train[:,0] == 0, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,1] == 0, (trueChildVisibleGeneration_train > 3))) +\n",
    "                np.count_nonzero(np.logical_and(y_train[:,2] == 0, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,3] == 0, (trueChildVisibleGeneration_train > 3)))) * classWeights_branch['higher_child_background_branch_weight']) + \\\n",
    "              ((np.count_nonzero(np.logical_and(y_train[:,0] == 2, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,1] == 2, (trueChildVisibleGeneration_train > 3))) +\n",
    "                np.count_nonzero(np.logical_and(y_train[:,2] == 2, (trueChildVisibleGeneration_train > 3))) + \n",
    "                np.count_nonzero(np.logical_and(y_train[:,3] == 2, (trueChildVisibleGeneration_train > 3)))) * classWeights_branch['higher_child_wrong_orientation_branch_weight'])\n",
    "\n",
    "print('this_signal:', this_signal)\n",
    "print('this_background:', this_background)\n",
    "print('this_wrong_orientation:', this_wrong_orientation)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Classifier weights!\n",
    "###########################################################\n",
    "\n",
    "# Calculate true/false classifier weights\n",
    "\n",
    "n_true_hierarchy_train = np.count_nonzero(trueParentChildLink_train == True)\n",
    "n_false_hierarchy_train = np.count_nonzero(trueParentChildLink_train == False)\n",
    "\n",
    "maxCounts_train = max(n_true_hierarchy_train, n_false_hierarchy_train)\n",
    "\n",
    "true_classifier_weight = float(maxCounts_train)/float(n_true_hierarchy_train)\n",
    "false_classifier_weight = float(maxCounts_train)/float(n_false_hierarchy_train)\n",
    "\n",
    "# Combine for classifier!\n",
    "\n",
    "n_secondary_child_true_links_train = np.count_nonzero(np.logical_and(trueParentChildLink_train == True, (trueChildVisibleGeneration_train == 3)))\n",
    "n_secondary_child_false_links_train = np.count_nonzero(np.logical_and(trueParentChildLink_train == False, (trueChildVisibleGeneration_train == 3)))\n",
    "\n",
    "n_higher_child_true_links_train = np.count_nonzero(np.logical_and(trueParentChildLink_train == True, (trueChildVisibleGeneration_train > 3)))\n",
    "n_higher_child_false_links_train = np.count_nonzero(np.logical_and(trueParentChildLink_train == False, (trueChildVisibleGeneration_train > 3)))\n",
    "\n",
    "secondary_child_true_links_train_weight = (n_true_hierarchy_train * 0.5) / n_secondary_child_true_links_train\n",
    "secondary_child_false_links_train_weight = (n_false_hierarchy_train * 0.5) / n_secondary_child_false_links_train\n",
    "\n",
    "higher_child_true_links_train_weight = (n_true_hierarchy_train * 0.5) / n_higher_child_true_links_train\n",
    "higher_child_false_links_train_weight = (n_false_hierarchy_train * 0.5) / n_higher_child_false_links_train\n",
    "\n",
    "classWeights_classifier = {\n",
    "    'secondary_child_true_links_train_weight'  : (secondary_child_true_links_train_weight * true_classifier_weight), \n",
    "    'secondary_child_false_links_train_weight' : (secondary_child_false_links_train_weight * false_classifier_weight),\n",
    "    'higher_child_true_links_train_weight'     : (higher_child_true_links_train_weight * true_classifier_weight),\n",
    "    'higher_child_false_links_train_weight'    : (higher_child_false_links_train_weight * false_classifier_weight)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77402fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Work out weights\n",
    "###########################################################\n",
    "\n",
    "classifier_weight = np.ones(trueParentChildLink_train.shape)\n",
    "classifier_weight[np.logical_and(trueParentChildLink_train == True, (trueChildVisibleGeneration_train == 3))] = classWeights_classifier['secondary_child_true_links_train_weight']\n",
    "classifier_weight[np.logical_and(trueParentChildLink_train == False, (trueChildVisibleGeneration_train == 3))] = classWeights_classifier['secondary_child_false_links_train_weight']\n",
    "classifier_weight[np.logical_and(trueParentChildLink_train == True, (trueChildVisibleGeneration_train > 3))] = classWeights_classifier['higher_child_true_links_train_weight']\n",
    "classifier_weight[np.logical_and(trueParentChildLink_train == False, (trueChildVisibleGeneration_train > 3))] = classWeights_classifier['higher_child_false_links_train_weight']\n",
    "\n",
    "print(np.sum(classifier_weight[np.logical_or(np.logical_and(trueParentChildLink_train == True, (trueChildVisibleGeneration_train == 3)), \n",
    "                                             np.logical_and(trueParentChildLink_train == False, (trueChildVisibleGeneration_train == 3)))]))     \n",
    "\n",
    "print(np.sum(classifier_weight[np.logical_or(np.logical_and(trueParentChildLink_train == True, (trueChildVisibleGeneration_train > 3)), \n",
    "                                             np.logical_and(trueParentChildLink_train == False, (trueChildVisibleGeneration_train > 3)))]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206045db",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Split it into four again\n",
    "###########################################################\n",
    "\n",
    "n_links = nLinks\n",
    "n_global_vars = 5\n",
    "n_link_vars = 21\n",
    "\n",
    "i_end_of_global_vars = n_global_vars\n",
    "i_target_start = variables_train.shape[1] - n_link_vars\n",
    "i_rest_start = n_global_vars\n",
    "i_rest_end = n_global_vars + (n_link_vars * (n_links - 1))\n",
    "\n",
    "# Training\n",
    "input_0_train_temp = variables_train\n",
    "input_1_train_temp = np.concatenate((input_0_train_temp[:,0:i_end_of_global_vars], input_0_train_temp[:,i_target_start:], input_0_train_temp[:,i_rest_start:i_rest_end]), axis=1)\n",
    "input_2_train_temp = np.concatenate((input_1_train_temp[:,0:i_end_of_global_vars], input_1_train_temp[:,i_target_start:], input_1_train_temp[:,i_rest_start:i_rest_end]), axis=1)\n",
    "input_3_train_temp = np.concatenate((input_2_train_temp[:,0:i_end_of_global_vars], input_2_train_temp[:,i_target_start:], input_2_train_temp[:,i_rest_start:i_rest_end]), axis=1)\n",
    "\n",
    "y_0_train_temp = y_train[:,0]\n",
    "y_1_train_temp = y_train[:,1]\n",
    "y_2_train_temp = y_train[:,2]\n",
    "y_3_train_temp = y_train[:,3]\n",
    "\n",
    "trueParentChildLink_train_temp = trueParentChildLink_train\n",
    "\n",
    "# Test\n",
    "input_0_test_temp = variables_test\n",
    "input_1_test_temp = np.concatenate((input_0_test_temp[:,0:i_end_of_global_vars], input_0_test_temp[:,i_target_start:], input_0_test_temp[:,i_rest_start:i_rest_end]), axis=1)\n",
    "input_2_test_temp = np.concatenate((input_1_test_temp[:,0:i_end_of_global_vars], input_1_test_temp[:,i_target_start:], input_1_test_temp[:,i_rest_start:i_rest_end]), axis=1)\n",
    "input_3_test_temp = np.concatenate((input_2_test_temp[:,0:i_end_of_global_vars], input_2_test_temp[:,i_target_start:], input_2_test_temp[:,i_rest_start:i_rest_end]), axis=1)\n",
    "\n",
    "y_0_test_temp = y_test[:,0]\n",
    "y_1_test_temp = y_test[:,1]\n",
    "y_2_test_temp = y_test[:,2]\n",
    "y_3_test_temp = y_test[:,3]\n",
    "\n",
    "trueParentChildLink_test_temp = trueParentChildLink_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a881dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Set up DataLoader\n",
    "###########################################################\n",
    "dataset_train = Datasets.TrackToTrackDataset(input_0_train_temp, input_1_train_temp, input_2_train_temp, input_3_train_temp, \\\n",
    "                                             y_train[:,0], y_train[:,1], y_train[:,2], y_train[:,3], trueParentChildLink_train, trueChildVisibleGeneration_train)\n",
    "loader_train = Datasets.DataLoader(dataset_train, shuffle=True, batch_size=BATCH_SIZE)    \n",
    "\n",
    "dataset_test = Datasets.TrackToTrackDataset(input_0_test_temp, input_1_test_temp, input_2_test_temp, input_3_test_temp, \\\n",
    "                                            y_test[:,0], y_test[:,1], y_test[:,2], y_test[:,3], trueParentChildLink_test, trueChildVisibleGeneration_test)\n",
    "loader_test = Datasets.DataLoader(dataset_test, shuffle=True, batch_size=BATCH_SIZE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Define the model and optimiser and compile the model\n",
    "###########################################################\n",
    "\n",
    "model_branch = Models.OrientationModel(nVariables, dropoutRate=DROPOUT_RATE)\n",
    "classifier_model = Models.ClassifierModel(nLinks * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Loss functions for training\n",
    "###########################################################\n",
    "    \n",
    "# Implement own weighting\n",
    "def loss_function_branch(pred, target, true_gen, weight_dict) :\n",
    "    \n",
    "    # Calculate weights\n",
    "    weights = torch.ones(true_gen.shape)\n",
    "    weights[torch.logical_and(target == 1, true_gen == 3)] = weight_dict['secondary_child_true_branch_weight']\n",
    "    weights[torch.logical_and(target == 0, true_gen == 3)] = weight_dict['secondary_child_background_branch_weight']\n",
    "    weights[torch.logical_and(target == 2, true_gen == 3)] = weight_dict['secondary_child_wrong_orientation_branch_weight']    \n",
    "    weights[torch.logical_and(target == 1, true_gen > 3)] = weight_dict['higher_child_true_branch_weight']\n",
    "    weights[torch.logical_and(target == 0, true_gen > 3)] = weight_dict['higher_child_background_branch_weight']\n",
    "    weights[torch.logical_and(target == 2, true_gen > 3)] = weight_dict['higher_child_wrong_orientation_branch_weight']    \n",
    "    \n",
    "    loss_func = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    loss = loss_func(pred, target)    \n",
    "    loss = loss * weights\n",
    "    loss = torch.sum(loss) / loss.shape[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Implement own weighting\n",
    "def loss_function_classifier(pred, target, true_gen, weight_dict) :\n",
    "\n",
    "    # I have to reshape so that I can do the weighting - sad.\n",
    "    pred = pred.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "    \n",
    "    # Calculate weights\n",
    "    weights = torch.ones(true_gen.shape)\n",
    "    weights[torch.logical_and(target == 1, true_gen == 3)] = weight_dict['secondary_child_true_links_train_weight']\n",
    "    weights[torch.logical_and(target == 0, true_gen == 3)] = weight_dict['secondary_child_false_links_train_weight']\n",
    "    weights[torch.logical_and(target == 1, true_gen > 3)] = weight_dict['higher_child_true_links_train_weight']\n",
    "    weights[torch.logical_and(target == 0, true_gen > 3)] = weight_dict['higher_child_false_links_train_weight']    \n",
    "    \n",
    "    # Annoyingly we have to change our target to a float\n",
    "    target = target.to(torch.float32)    \n",
    "    \n",
    "    # Use BCE loss\n",
    "    loss_func = torch.nn.BCELoss(weight=weights)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = loss_func(pred, target)    \n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c03c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Training and testing\n",
    "######################\n",
    "\n",
    "# Optimiser\n",
    "optimiser = torch.optim.Adam(itertools.chain(model_branch.parameters(), classifier_model.parameters()), lr=LEARNING_RATE)\n",
    "#loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "# Scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.1, patience=3, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "# Put here some metrics\n",
    "training_epoch = []\n",
    "training_loss_0 = []\n",
    "training_loss_1 = []\n",
    "training_loss_2 = []\n",
    "training_loss_3 = []\n",
    "training_classification_loss = []\n",
    "training_accuracy = []\n",
    "training_positive_as_positive_rate = []\n",
    "training_positive_as_negative_rate = []\n",
    "training_negative_as_negative_rate = []\n",
    "training_negative_as_positive_rate = []\n",
    "\n",
    "testing_epoch = []\n",
    "testing_loss_0 = []\n",
    "testing_loss_1 = []\n",
    "testing_loss_2 = []\n",
    "testing_loss_3 = []\n",
    "testing_classification_loss = []\n",
    "testing_accuracy = []\n",
    "testing_positive_as_positive_rate = []\n",
    "testing_positive_as_negative_rate = []\n",
    "testing_negative_as_negative_rate = []\n",
    "testing_negative_as_positive_rate = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    ######################\n",
    "    # Training \n",
    "    ######################    \n",
    "    model_branch.train()\n",
    "    classifier_model.train()\n",
    "    \n",
    "    # Iterate in batches over the training dataset.                        \n",
    "    for features_0, features_1, features_2, features_3, \\\n",
    "        target_0, target_1, target_2, target_3, target, new_gen in loader_train :  \n",
    "        \n",
    "        # Skip incomplete batches\n",
    "        if (target.shape[0] != BATCH_SIZE) :\n",
    "            continue        \n",
    "            \n",
    "        # Get predictions\n",
    "        pred_0 = model_branch(features_0)\n",
    "        pred_1 = model_branch(features_1)\n",
    "        pred_2 = model_branch(features_2)\n",
    "        pred_3 = model_branch(features_3)\n",
    "        classifier_pred = classifier_model(torch.concatenate((pred_0, pred_1, pred_2, pred_3), axis=1))\n",
    "        \n",
    "        # Get loss\n",
    "        target_0 = target_0.to(torch.long)\n",
    "        target_1 = target_1.to(torch.long)\n",
    "        target_2 = target_2.to(torch.long)\n",
    "        target_3 = target_3.to(torch.long)\n",
    "        classifier_target = target.to(torch.long).reshape(-1,1)\n",
    "        loss_0 = loss_function_branch(pred_0, target_0, new_gen, classWeights_branch)        \n",
    "        loss_1 = loss_function_branch(pred_1, target_1, new_gen, classWeights_branch)\n",
    "        loss_2 = loss_function_branch(pred_2, target_2, new_gen, classWeights_branch) \n",
    "        loss_3 = loss_function_branch(pred_3, target_3, new_gen, classWeights_branch) \n",
    "        classifier_loss = loss_function_classifier(classifier_pred, classifier_target, new_gen, classWeights_classifier)\n",
    "        total_loss = loss_0 + loss_1 + loss_2 + loss_3 + classifier_loss\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimiser.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    ######################\n",
    "    # Validation metrics \n",
    "    ######################\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Begin testing mode\n",
    "        model_branch.eval()\n",
    "        classifier_model.eval()\n",
    "        \n",
    "        # Initialise metrics        \n",
    "        total_batches_train = 0        \n",
    "        total_loss_0_train = 0\n",
    "        true_scores_0_train = []\n",
    "        background_scores_0_train = []\n",
    "        wrong_orientation_scores_0_train = []\n",
    "        total_loss_1_train = 0\n",
    "        true_scores_1_train = []\n",
    "        background_scores_1_train = []\n",
    "        wrong_orientation_scores_1_train = []        \n",
    "        total_loss_2_train = 0\n",
    "        true_scores_2_train = []\n",
    "        background_scores_2_train = []\n",
    "        wrong_orientation_scores_2_train = []        \n",
    "        total_loss_3_train = 0\n",
    "        true_scores_3_train = []\n",
    "        background_scores_3_train = []\n",
    "        wrong_orientation_scores_3_train = []        \n",
    "        total_classification_loss_train = 0\n",
    "        positive_scores_train = []\n",
    "        negative_scores_train = []\n",
    "        \n",
    "        total_batches_test = 0        \n",
    "        total_loss_0_test = 0\n",
    "        true_scores_0_test = []\n",
    "        background_scores_0_test = []\n",
    "        wrong_orientation_scores_0_test = []            \n",
    "        total_loss_1_test = 0\n",
    "        true_scores_1_test = []\n",
    "        background_scores_1_test = []\n",
    "        wrong_orientation_scores_1_test = []        \n",
    "        total_loss_2_test = 0\n",
    "        true_scores_2_test = []\n",
    "        background_scores_2_test = []\n",
    "        wrong_orientation_scores_2_test = []        \n",
    "        total_loss_3_test = 0\n",
    "        true_scores_3_test = []\n",
    "        background_scores_3_test = []\n",
    "        wrong_orientation_scores_3_test = []                \n",
    "        total_classification_loss_test = 0\n",
    "        positive_scores_test = []\n",
    "        negative_scores_test = []        \n",
    "                \n",
    "        # Iterate in batches over the training dataset.                        \n",
    "        for features_0_train, features_1_train, features_2_train, features_3_train, \\\n",
    "            target_0_train, target_1_train, target_2_train, target_3_train, target_train, new_gen_train in loader_train :  \n",
    "\n",
    "            # Skip incomplete batches\n",
    "            if (target_train.shape[0] != BATCH_SIZE) :\n",
    "                continue        \n",
    "\n",
    "            # Get predictions\n",
    "            pred_0_train = model_branch(features_0_train)\n",
    "            pred_1_train = model_branch(features_1_train)\n",
    "            pred_2_train = model_branch(features_2_train)\n",
    "            pred_3_train = model_branch(features_3_train)\n",
    "            classifier_pred_train = classifier_model(torch.concatenate((pred_0_train, pred_1_train, pred_2_train, pred_3_train), axis=1))\n",
    "\n",
    "            # Get loss\n",
    "            target_0_train = target_0_train.to(torch.long)\n",
    "            target_1_train = target_1_train.to(torch.long)\n",
    "            target_2_train = target_2_train.to(torch.long)\n",
    "            target_3_train = target_3_train.to(torch.long)            \n",
    "            classifier_target_train = target_train.to(torch.long).reshape(-1,1)\n",
    "            loss_0_train = loss_function_branch(pred_0_train, target_0_train, new_gen_train, classWeights_branch)   \n",
    "            loss_1_train = loss_function_branch(pred_1_train, target_1_train, new_gen_train, classWeights_branch)\n",
    "            loss_2_train = loss_function_branch(pred_2_train, target_2_train, new_gen_train, classWeights_branch)\n",
    "            loss_3_train = loss_function_branch(pred_3_train, target_3_train, new_gen_train, classWeights_branch)            \n",
    "            classifier_loss_train = loss_function_classifier(classifier_pred_train, classifier_target_train, new_gen_train, classWeights_classifier)\n",
    "            total_loss_train = loss_0_train + loss_1_train + loss_2_train + loss_3_train + classifier_loss_train\n",
    "            \n",
    "            # Add to our metrics\n",
    "            total_batches_train += 1            \n",
    "            total_loss_0_train += loss_0_train.item()  \n",
    "            true_scores_0_train.extend(np.array(pred_0_train.tolist())[torch.column_stack((target_0_train == 1, target_0_train == 1, target_0_train == 1)).numpy()].reshape(-1, 3))\n",
    "            background_scores_0_train.extend(np.array(pred_0_train.tolist())[torch.column_stack((target_0_train == 0, target_0_train == 0, target_0_train == 0)).numpy()].reshape(-1, 3))\n",
    "            wrong_orientation_scores_0_train.extend(np.array(pred_0_train.tolist())[torch.column_stack((target_0_train == 2, target_0_train == 2, target_0_train == 2)).numpy()].reshape(-1, 3))\n",
    "            total_loss_1_train += loss_1_train.item()  \n",
    "            true_scores_1_train.extend(np.array(pred_1_train.tolist())[torch.column_stack((target_1_train == 1, target_1_train == 1, target_1_train == 1)).numpy()].reshape(-1, 3))\n",
    "            background_scores_1_train.extend(np.array(pred_1_train.tolist())[torch.column_stack((target_1_train == 0, target_1_train == 0, target_1_train == 0)).numpy()].reshape(-1, 3))\n",
    "            wrong_orientation_scores_1_train.extend(np.array(pred_1_train.tolist())[torch.column_stack((target_1_train == 2, target_1_train == 2, target_1_train == 2)).numpy()].reshape(-1, 3))            \n",
    "            total_loss_2_train += loss_2_train.item()  \n",
    "            true_scores_2_train.extend(np.array(pred_2_train.tolist())[torch.column_stack((target_2_train == 1, target_2_train == 1, target_2_train == 1)).numpy()].reshape(-1, 3))\n",
    "            background_scores_2_train.extend(np.array(pred_2_train.tolist())[torch.column_stack((target_2_train == 0, target_2_train == 0, target_2_train == 0)).numpy()].reshape(-1, 3))\n",
    "            wrong_orientation_scores_2_train.extend(np.array(pred_2_train.tolist())[torch.column_stack((target_2_train == 2, target_2_train == 2, target_2_train == 2)).numpy()].reshape(-1, 3))            \n",
    "            total_loss_3_train += loss_3_train.item()  \n",
    "            true_scores_3_train.extend(np.array(pred_3_train.tolist())[torch.column_stack((target_3_train == 1, target_3_train == 1, target_3_train == 1)).numpy()].reshape(-1, 3))\n",
    "            background_scores_3_train.extend(np.array(pred_3_train.tolist())[torch.column_stack((target_3_train == 0, target_3_train == 0, target_3_train == 0)).numpy()].reshape(-1, 3))\n",
    "            wrong_orientation_scores_3_train.extend(np.array(pred_3_train.tolist())[torch.column_stack((target_3_train == 2, target_3_train == 2, target_3_train == 2)).numpy()].reshape(-1, 3))              \n",
    "            total_classification_loss_train += classifier_loss_train.item()\n",
    "            positive_scores_train.extend(np.array(classifier_pred_train.tolist())[(classifier_target_train == 1).numpy()].reshape(-1))\n",
    "            negative_scores_train.extend(np.array(classifier_pred_train.tolist())[(classifier_target_train == 0).numpy()].reshape(-1))\n",
    "            \n",
    "        training_epoch.append(epoch)            \n",
    "            \n",
    "        # Iterate in batches over the testing dataset.                        \n",
    "        for features_0_test, features_1_test, features_2_test, features_3_test, \\\n",
    "            target_0_test, target_1_test, target_2_test, target_3_test, target_test, new_gen_test in loader_test :  \n",
    "\n",
    "            # Skip incomplete batches\n",
    "            if (target_test.shape[0] != BATCH_SIZE) :\n",
    "                continue        \n",
    "\n",
    "            # Get predictions\n",
    "            pred_0_test = model_branch(features_0_test)\n",
    "            pred_1_test = model_branch(features_1_test)\n",
    "            pred_2_test = model_branch(features_2_test)\n",
    "            pred_3_test = model_branch(features_3_test)            \n",
    "            classifier_pred_test = classifier_model(torch.concatenate((pred_0_test, pred_1_test, pred_2_test, pred_3_test), axis=1))\n",
    "\n",
    "            # Get loss\n",
    "            target_0_test = target_0_test.to(torch.long)\n",
    "            target_1_test = target_1_test.to(torch.long)\n",
    "            target_2_test = target_2_test.to(torch.long)\n",
    "            target_3_test = target_3_test.to(torch.long)            \n",
    "            classifier_target_test = target_test.to(torch.long).reshape(-1,1)\n",
    "            loss_0_test = loss_function_branch(pred_0_test, target_0_test, new_gen_test, classWeights_branch)   \n",
    "            loss_1_test = loss_function_branch(pred_1_test, target_1_test, new_gen_test, classWeights_branch)\n",
    "            loss_2_test = loss_function_branch(pred_2_test, target_2_test, new_gen_test, classWeights_branch)\n",
    "            loss_3_test = loss_function_branch(pred_3_test, target_3_test, new_gen_test, classWeights_branch)            \n",
    "            classifier_loss_test = loss_function_classifier(classifier_pred_test, classifier_target_test, new_gen_test, classWeights_classifier)\n",
    "            total_loss_test = loss_0_test + loss_1_test + loss_2_test + loss_3_test + classifier_loss_test\n",
    "            \n",
    "            # Add to our metrics\n",
    "            total_batches_test += 1          \n",
    "            total_loss_0_test += loss_0_test.item()             \n",
    "            true_scores_0_test.extend(np.array(pred_0_test.tolist())[torch.column_stack((target_0_test == 1, target_0_test == 1, target_0_test == 1)).numpy()].reshape(-1, 3))\n",
    "            background_scores_0_test.extend(np.array(pred_0_test.tolist())[torch.column_stack((target_0_test == 0, target_0_test == 0, target_0_test == 0)).numpy()].reshape(-1, 3))\n",
    "            wrong_orientation_scores_0_test.extend(np.array(pred_0_test.tolist())[torch.column_stack((target_0_test == 2, target_0_test == 2, target_0_test == 2)).numpy()].reshape(-1, 3))            \n",
    "            total_loss_1_test += loss_1_test.item()             \n",
    "            true_scores_1_test.extend(np.array(pred_1_test.tolist())[torch.column_stack((target_1_test == 1, target_1_test == 1, target_1_test == 1)).numpy()].reshape(-1, 3))\n",
    "            background_scores_1_test.extend(np.array(pred_1_test.tolist())[torch.column_stack((target_1_test == 0, target_1_test == 0, target_1_test == 0)).numpy()].reshape(-1, 3))\n",
    "            wrong_orientation_scores_1_test.extend(np.array(pred_1_test.tolist())[torch.column_stack((target_1_test == 2, target_1_test == 2, target_1_test == 2)).numpy()].reshape(-1, 3))            \n",
    "            total_loss_2_test += loss_2_test.item()             \n",
    "            true_scores_2_test.extend(np.array(pred_2_test.tolist())[torch.column_stack((target_2_test == 1, target_2_test == 1, target_2_test == 1)).numpy()].reshape(-1, 3))\n",
    "            background_scores_2_test.extend(np.array(pred_2_test.tolist())[torch.column_stack((target_2_test == 0, target_2_test == 0, target_2_test == 0)).numpy()].reshape(-1, 3))\n",
    "            wrong_orientation_scores_2_test.extend(np.array(pred_2_test.tolist())[torch.column_stack((target_2_test == 2, target_2_test == 2, target_2_test == 2)).numpy()].reshape(-1, 3))            \n",
    "            total_loss_2_test += loss_2_test.item()             \n",
    "            true_scores_2_test.extend(np.array(pred_2_test.tolist())[torch.column_stack((target_2_test == 1, target_2_test == 1, target_2_test == 1)).numpy()].reshape(-1, 3))\n",
    "            background_scores_2_test.extend(np.array(pred_2_test.tolist())[torch.column_stack((target_2_test == 0, target_2_test == 0, target_2_test == 0)).numpy()].reshape(-1, 3))\n",
    "            wrong_orientation_scores_2_test.extend(np.array(pred_2_test.tolist())[torch.column_stack((target_2_test == 2, target_2_test == 2, target_2_test == 2)).numpy()].reshape(-1, 3))                        \n",
    "            total_classification_loss_test += classifier_loss_test.item()\n",
    "            positive_scores_test.extend(np.array(classifier_pred_test.tolist())[(classifier_target_test == 1).numpy()].reshape(-1))\n",
    "            negative_scores_test.extend(np.array(classifier_pred_test.tolist())[(classifier_target_test == 0).numpy()].reshape(-1))            \n",
    "            \n",
    "        testing_epoch.append(epoch)\n",
    "    \n",
    "    ##########################\n",
    "    # Calc metrics for epoch \n",
    "    ##########################   \n",
    "    # train\n",
    "    optimal_threshold_train, maximum_accuracy_train = TrainingMetrics.calculate_accuracy(torch.tensor(positive_scores_train), torch.tensor(negative_scores_train)) \n",
    "    # test\n",
    "    optimal_threshold_test, maximum_accuracy_test = TrainingMetrics.calculate_accuracy(torch.tensor(positive_scores_test), torch.tensor(negative_scores_test))\n",
    "\n",
    "    # train\n",
    "    positive_as_positive_train = np.count_nonzero(np.array(positive_scores_train) > optimal_threshold_train)\n",
    "    positive_as_negative_train = np.count_nonzero(np.array(positive_scores_train) < optimal_threshold_train)\n",
    "    negative_as_positive_train = np.count_nonzero(np.array(negative_scores_train) > optimal_threshold_train)\n",
    "    negative_as_negative_train = np.count_nonzero(np.array(negative_scores_train) < optimal_threshold_train)\n",
    "    # test\n",
    "    positive_as_positive_test = np.count_nonzero(np.array(positive_scores_test) > optimal_threshold_test)\n",
    "    positive_as_negative_test = np.count_nonzero(np.array(positive_scores_test) < optimal_threshold_test)\n",
    "    negative_as_positive_test = np.count_nonzero(np.array(negative_scores_test) > optimal_threshold_test)\n",
    "    negative_as_negative_test = np.count_nonzero(np.array(negative_scores_test) < optimal_threshold_test)\n",
    "    \n",
    "    # train\n",
    "    positive_as_positive_fraction_train = float(positive_as_positive_train) / float(positive_as_positive_train + positive_as_negative_train)\n",
    "    positive_as_negative_fraction_train = float(positive_as_negative_train) / float(positive_as_positive_train + positive_as_negative_train)\n",
    "    negative_as_positive_fraction_train = float(negative_as_positive_train) / float(negative_as_positive_train + negative_as_negative_train)\n",
    "    negative_as_negative_fraction_train = float(negative_as_negative_train) / float(negative_as_positive_train + negative_as_negative_train)\n",
    "    # test\n",
    "    positive_as_positive_fraction_test = float(positive_as_positive_test) / float(positive_as_positive_test + positive_as_negative_test)\n",
    "    positive_as_negative_fraction_test = float(positive_as_negative_test) / float(positive_as_positive_test + positive_as_negative_test)\n",
    "    negative_as_positive_fraction_test = float(negative_as_positive_test) / float(negative_as_positive_test + negative_as_negative_test)\n",
    "    negative_as_negative_fraction_test = float(negative_as_negative_test) / float(negative_as_positive_test + negative_as_negative_test)\n",
    "    \n",
    "    # Add to our metrics\n",
    "    training_loss_0.append(float(total_loss_0_train) / total_batches_train)\n",
    "    training_loss_1.append(float(total_loss_1_train) / total_batches_train)\n",
    "    training_classification_loss.append(float(total_classification_loss_train) / total_batches_train)\n",
    "    training_accuracy.append(maximum_accuracy_train)\n",
    "    training_positive_as_positive_rate.append(positive_as_positive_fraction_train)\n",
    "    training_positive_as_negative_rate.append(positive_as_negative_fraction_train)\n",
    "    training_negative_as_negative_rate.append(negative_as_negative_fraction_train)\n",
    "    training_negative_as_positive_rate.append(negative_as_positive_fraction_train)\n",
    "    \n",
    "    testing_loss_0.append(float(total_loss_0_test) / total_batches_test)\n",
    "    testing_loss_1.append(float(total_loss_1_test) / total_batches_test)\n",
    "    testing_classification_loss.append(float(total_classification_loss_test) / total_batches_test)\n",
    "    testing_positive_as_positive_rate.append(positive_as_positive_fraction_test)\n",
    "    testing_positive_as_negative_rate.append(positive_as_negative_fraction_test)\n",
    "    testing_negative_as_negative_rate.append(negative_as_negative_fraction_test)\n",
    "    testing_negative_as_positive_rate.append(negative_as_positive_fraction_test)    \n",
    "    \n",
    "    # Do some prints\n",
    "    print('----------------------------------------')\n",
    "    print('Epoch:', epoch)\n",
    "    print('----------------------------------------')\n",
    "    print('training_classification_loss:', round(training_classification_loss[-1], 2))\n",
    "    print('----')\n",
    "    print('optimal_threshold_train:', optimal_threshold_train)\n",
    "    print('accuracy_train:', str(round(maximum_accuracy_train.item(), 2)) +'%')\n",
    "    print('positive_as_positive_fraction_train:', str(round(positive_as_positive_fraction_train * 100.0, 2)) + '%')\n",
    "    print('positive_as_negative_fraction_train:', str(round(positive_as_negative_fraction_train * 100.0, 2)) + '%')\n",
    "    print('negative_as_negative_fraction_train:', str(round(negative_as_negative_fraction_train * 100.0, 2)) + '%')\n",
    "    print('negative_as_positive_fraction_train:', str(round(negative_as_positive_fraction_train * 100.0, 2)) + '%')\n",
    "    print('----')\n",
    "    print('testing_classification_loss:', round(testing_classification_loss[-1], 2))\n",
    "    print('----')\n",
    "    print('optimal_threshold_test:', optimal_threshold_test)\n",
    "    print('accuracy_test:', str(round(maximum_accuracy_test.item(), 2)) +'%')\n",
    "    print('positive_as_positive_fraction_test:', str(round(positive_as_positive_fraction_test * 100.0, 2)) + '%')\n",
    "    print('positive_as_negative_fraction_test:', str(round(positive_as_negative_fraction_test * 100.0, 2)) + '%')\n",
    "    print('negative_as_negative_fraction_test:', str(round(negative_as_negative_fraction_test * 100.0, 2)) + '%')\n",
    "    print('negative_as_positive_fraction_test:', str(round(negative_as_positive_fraction_test * 100.0, 2)) + '%')\n",
    "    print('----')\n",
    "    TrainingMetrics.plot_scores_branch(background_scores_0_train, true_scores_0_train, wrong_orientation_scores_0_train, \\\n",
    "                                       background_scores_0_test, true_scores_0_test, wrong_orientation_scores_0_test, 0)\n",
    "    TrainingMetrics.plot_scores_branch(background_scores_0_train, true_scores_0_train, wrong_orientation_scores_0_train, \\\n",
    "                                       background_scores_0_test, true_scores_0_test, wrong_orientation_scores_0_test, 1)\n",
    "    TrainingMetrics.plot_scores_branch(background_scores_0_train, true_scores_0_train, wrong_orientation_scores_0_train, \\\n",
    "                                       background_scores_0_test, true_scores_0_test, wrong_orientation_scores_0_test, 2)   \n",
    "\n",
    "#     TrainingMetrics.plot_scores_branch(background_scores_1_train, true_scores_1_train, wrong_orientation_scores_1_train, \\\n",
    "#                                        background_scores_1_test, true_scores_1_test, wrong_orientation_scores_1_test, 0)\n",
    "#     TrainingMetrics.plot_scores_branch(background_scores_1_train, true_scores_1_train, wrong_orientation_scores_1_train, \\\n",
    "#                                        background_scores_1_test, true_scores_1_test, wrong_orientation_scores_1_test, 1)\n",
    "#     TrainingMetrics.plot_scores_branch(background_scores_1_train, true_scores_1_train, wrong_orientation_scores_1_train, \\\n",
    "#                                        background_scores_1_test, true_scores_1_test, wrong_orientation_scores_1_test, 2)    \n",
    "    TrainingMetrics.plot_scores_classifier(positive_scores_train, negative_scores_train, positive_scores_test, negative_scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c4482",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print metrics showing evolution   \n",
    "TrainingMetrics.plot_loss_evolution(training_epoch, training_loss_0, testing_loss_0, 'Loss - model_branch_0')\n",
    "TrainingMetrics.plot_loss_evolution(training_epoch, training_loss_1, testing_loss_1, 'Loss - model_branch_1')\n",
    "TrainingMetrics.plot_loss_evolution(training_epoch, training_classification_loss, testing_classification_loss, 'Loss - classifier')\n",
    "TrainingMetrics.plot_edge_rate(training_epoch, training_positive_as_positive_rate, training_positive_as_negative_rate, testing_positive_as_positive_rate, testing_positive_as_negative_rate, True)\n",
    "TrainingMetrics.plot_edge_rate(testing_epoch, training_negative_as_negative_rate, training_negative_as_positive_rate, testing_negative_as_negative_rate, testing_negative_as_positive_rate, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7dfa7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Confusion matrices!\n",
    "######################\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Begin testing mode\n",
    "    model_branch.eval()\n",
    "    classifier_model.eval()\n",
    "    # Get predictions\n",
    "    pred_0_test = model_branch(torch.tensor(input_0_test_temp, dtype=torch.float))\n",
    "    pred_1_test = model_branch(torch.tensor(input_1_test_temp, dtype=torch.float))\n",
    "    pred_2_test = model_branch(torch.tensor(input_2_test_temp, dtype=torch.float))\n",
    "    pred_3_test = model_branch(torch.tensor(input_3_test_temp, dtype=torch.float))    \n",
    "    classifier_pred_test = classifier_model(torch.concatenate((pred_0_test, pred_1_test, pred_2_test, pred_3_test), axis=1)).reshape(-1)\n",
    "\n",
    "    neg_scores_final_test = np.array(classifier_pred_test.tolist())[trueParentChildLink_test == 0].reshape(-1)\n",
    "    pos_scores_final_test = np.array(classifier_pred_test.tolist())[trueParentChildLink_test == 1].reshape(-1)\n",
    "    \n",
    "    TrainingMetrics.plot_roc_curve(torch.tensor(pos_scores_final_test), torch.tensor(neg_scores_final_test))\n",
    "    TrainingMetrics.draw_confusion_with_threshold(classifier_pred_test, trueParentChildLink_test, 0.5)\n",
    "    TrainingMetrics.draw_confusion_with_threshold(classifier_pred_test, trueParentChildLink_test, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Save the model\n",
    "###################### \n",
    "sm_branch = torch.jit.script(model_branch)\n",
    "sm_branch.save(branchModelPath)\n",
    "\n",
    "sm_classifier = torch.jit.script(classifier_model)\n",
    "sm_classifier.save(classifierModelPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
