{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86c2366-711e-4f59-8cfd-f6ea792c58b4",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Training\n",
    "\n",
    "written by Isobel Mawby (i.mawby1@lancaster.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d788b-f09d-43b7-8576-743a514d4ea5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Imports\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f2d46-251a-4563-a0a9-b65f2e8dbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import Datasets\n",
    "import TrainingMetrics\n",
    "import Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34525e28-da6d-4c1b-803c-7b8ed766e29b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Set device\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e20fe-9f61-43e6-a035-d6e0dc123f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c93107-ea0e-4739-9aa0-da707378cfca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Config\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59eccf-7126-45ac-8cc4-1fec87f845f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_FRACTION = 0.75\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "N_EPOCHS = 1\n",
    "ALPHA = 2.0     # Loss scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69112f78-74f1-4d43-974e-73b3a424d084",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    File \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6809844-d828-419a-ba7d-e00963ec30d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = sys.path[0] + '/models/SplitPointModel_UVW'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02c928-d335-4261-abbe-5f784af43a9c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Pull out things from file\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60c52b-71eb-4b19-82dc-f2803c9cf472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = Datasets.get_split_point_datasets(device, TRAINING_FRACTION)\n",
    "\n",
    "print('Input(train):', train_dataset.input.shape)\n",
    "print('Truth(train):', train_dataset.labels.shape)\n",
    "print('Contaminated(train):', train_dataset.is_contaminated.shape)\n",
    "print('')\n",
    "print('Input(test):', test_dataset.input.shape)\n",
    "print('Truth(test):', test_dataset.labels.shape)\n",
    "print('Contaminated(test):', test_dataset.is_contaminated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f96b6-8c4e-4737-a014-03026946e0ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "     Do not train on showers, they confuse things\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802cb2f-9f57-413c-a9a3-44ce6e5c1fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_train = train_dataset.is_contaminated != 2\n",
    "mask_test = test_dataset.is_contaminated != 2\n",
    "\n",
    "train_dataset.input = train_dataset.input.unsqueeze(1)[mask_train]\n",
    "train_dataset.labels = train_dataset.labels.unsqueeze(1)[mask_train]\n",
    "train_dataset.is_contaminated = train_dataset.is_contaminated[mask_train].reshape(-1,1)\n",
    "\n",
    "test_dataset.input = test_dataset.input.unsqueeze(1)[mask_test]\n",
    "test_dataset.labels = test_dataset.labels.unsqueeze(1)[mask_test]\n",
    "test_dataset.is_contaminated = test_dataset.is_contaminated[mask_test].reshape(-1,1)\n",
    "\n",
    "print('Input(train):', train_dataset.input.shape)\n",
    "print('Truth(train):', train_dataset.labels.shape)\n",
    "print('Contaminated(train):', train_dataset.is_contaminated.shape)\n",
    "print('')\n",
    "print('Input(test):', test_dataset.input.shape)\n",
    "print('Truth(test):', test_dataset.labels.shape)\n",
    "print('Contaminated(test):', test_dataset.is_contaminated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef45ab-1ea4-41e2-b103-5667680b7dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for class_index in TrainingMetrics.contamination_labels :\n",
    "    print(f'{TrainingMetrics.contamination_strings[class_index]}: {torch.count_nonzero(train_dataset.labels == class_index).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434827d1-f9a6-4ca5-8a88-0eda0ef228bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0, generator=torch.Generator(device='cpu'))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=0, generator=torch.Generator(device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04773113-8412-4513-8cfb-43fddb13fef5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Class Weights\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26641bd5-fc01-443b-979f-00ac0a7dd977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nFalse = torch.count_nonzero(train_dataset.labels == 0).item()\n",
    "nTrue = torch.count_nonzero(train_dataset.labels == 1).item()\n",
    "maxValue = max(nTrue, nFalse)\n",
    "\n",
    "if (nFalse == 0) or (nTrue == 0):\n",
    "    raise Exception(\"Training class has zero samples!\")\n",
    "\n",
    "class_weights = torch.tensor([float(maxValue)/float(nFalse), float(maxValue)/float(nTrue)])\n",
    "\n",
    "print('nTrue:', nTrue)\n",
    "print('nFalse:', nFalse)\n",
    "print('weights:', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0603032-4354-410f-940f-e7d4e96ab22f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Setup training\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11c52b-148c-4ce3-87b0-f6320fe59cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get our model\n",
    "_, _, n_features = train_dataset.input.shape\n",
    "model = Models.ConvEncoderDecoder(device, num_features=n_features)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimiser = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimiser, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804afd8-a038-41e4-9e17-545a0572bcf9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Let's start training!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b774380-5961-43ef-9183-20195838be82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_function(prediction, truth, class_weights) :\n",
    "    b, l, _ = truth.shape\n",
    "    \n",
    "    # do not reduce, so we can handle masking\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    # Get loss_1\n",
    "    loss_1 = loss_func(prediction, truth.float())\n",
    "\n",
    "    # Apply weighting\n",
    "    loss_1[truth == 0] *= class_weights[0]\n",
    "    loss_1[truth == 1] *= class_weights[1]  \n",
    "    loss_1 = loss_1.squeeze(2)\n",
    "    loss_1 = (loss_1.sum(dim=1)/l)\n",
    "    \n",
    "    # Want to make sure that model doesn't produce an excessive number of split points\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "    n_true_splits =  truth.squeeze(2).sum(dim=1).float()\n",
    "    n_pred_splits =  prediction.squeeze(2).sum(dim=1).float()\n",
    "    diff = n_pred_splits - n_true_splits\n",
    "    relu = nn.ReLU()\n",
    "    loss_2 = relu(diff)\n",
    "    \n",
    "    loss_combined = loss_1 + (loss_2 * ALPHA)\n",
    "    loss_combined = loss_combined.sum() / b\n",
    "    \n",
    "    return loss_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07b462-1461-4ca2-a055-58022946eef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize progress bar for tracking epochs\n",
    "pbar = trange(0, N_EPOCHS, leave=True, desc=\"Epoch\")\n",
    "\n",
    "# Loggers for training and testing\n",
    "train_av_loss_logger = []\n",
    "test_av_loss_logger = []\n",
    "train_acc_logger = [[],[]]\n",
    "test_acc_logger = [[],[]]\n",
    "\n",
    "# Loop over each epoch\n",
    "for epoch in pbar:\n",
    "    # Metrics\n",
    "    train_loss_count = 0\n",
    "    test_loss_count = 0\n",
    "    train_acc_count = [0, 0]                                                                                                                                                                                \n",
    "    test_acc_count = [0, 0]                                                                                                                                                                                 \n",
    "    counts_train = [0, 0]                                                                                                                                                                                   \n",
    "    counts_test = [0, 0]   \n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Loop over each batch in the training dataset\n",
    "    for x_train, label_train, _ in tqdm(train_dataloader, desc=\"Training\", leave=True):\n",
    "        # Make prediction\n",
    "        pred = model(x_train)\n",
    "        \n",
    "        # Compute the loss using cross-entropy loss\n",
    "        loss = loss_function(pred, label_train, class_weights)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "            \n",
    "    tt = time.time()\n",
    "    \n",
    "    # Update learning rate\n",
    "    before_lr = optimiser.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimiser.param_groups[0][\"lr\"]\n",
    "    print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))   \n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()  \n",
    "    \n",
    "    # Loop over each batch in the training dataset\n",
    "    for x_train, label_train, _ in tqdm(train_dataloader, desc=\"Validation(Train)\", leave=True):\n",
    "        # Make prediction\n",
    "        pred = model(x_train)\n",
    "        \n",
    "        # Compute the loss using cross-entropy loss\n",
    "        loss = loss_function(pred, label_train, class_weights)\n",
    "        train_loss_count += loss.item()\n",
    "\n",
    "        # Apply sigmoid for inference\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # Update training accuracy\n",
    "        label_train = label_train.reshape(-1)\n",
    "        pred = pred.reshape(-1)\n",
    "        pred = torch.round(pred)\n",
    "        \n",
    "        for i in range(2) :\n",
    "            train_acc_count[i] += torch.count_nonzero(torch.logical_and(torch.isclose(pred, label_train.float()), label_train == i))\n",
    "            counts_train[i] += torch.count_nonzero(label_train == i)      \n",
    "    \n",
    "    # Loop over each batch in the testing dataset\n",
    "    with torch.no_grad():\n",
    "        for x_test, label_test, _ in tqdm(test_dataloader, desc=\"Validation(Test)\", leave=True):    \n",
    "\n",
    "            # Make prediction\n",
    "            pred = model(x_test)\n",
    "\n",
    "            # Compute the loss using cross-entropy loss\n",
    "            loss = loss_function(pred, label_test, class_weights)  \n",
    "            test_loss_count += loss.item()\n",
    "\n",
    "            # Apply sigmoid for inference\n",
    "            pred = torch.sigmoid(pred)\n",
    "            \n",
    "            # Update training accuracy\n",
    "            label_test = label_test.reshape(-1)\n",
    "            pred = pred.reshape(-1)\n",
    "            pred = torch.round(pred)\n",
    "        \n",
    "            for i in range(2) :\n",
    "                test_acc_count[i] += torch.count_nonzero(torch.logical_and(torch.isclose(pred, label_test.float()), label_test == i))\n",
    "                counts_test[i] += torch.count_nonzero(label_test == i)                            \n",
    "            \n",
    "        tv = time.time()            \n",
    "        \n",
    "        # Prints for Epoch\n",
    "        train_av_loss_logger.append(train_loss_count / len(train_dataloader))\n",
    "        test_av_loss_logger.append(test_loss_count / len(test_dataloader))\n",
    "        print(f'Training Time: {tt-t0:.3f} s')\n",
    "        print(f'Validation Time: {tv-tt:.3f} s')\n",
    "        print('')\n",
    "        for i in range(2) :\n",
    "            train_acc = (train_acc_count[i] / counts_train[i]).item()\n",
    "            test_acc = (test_acc_count[i] / counts_test[i]).item()\n",
    "            \n",
    "            print(f'train/test accuracy for class {i}: {train_acc*100.0:.2f}%, {test_acc*100.0:.2f}%')\n",
    "            train_acc_logger[i].append(train_acc)\n",
    "            test_acc_logger[i].append(test_acc)             \n",
    "            \n",
    "        # Save for each epoch\n",
    "        model_cpu = model.to('cpu').eval()                                                                                                                                                \n",
    "        sm = torch.jit.script(model_cpu)                                                                                                                                                       \n",
    "        sm.save(f\"{model_path}_alpha_{ALPHA}_epoch_{epoch}.pt\")                                                                                                                                          \n",
    "        torch.save(model_cpu.state_dict(), f\"{model_path}_alpha_{ALPHA}_epoch_{epoch}.pkl\")                                                                                                         \n",
    "        model = model.to(device)                                                                                                                                                          \n",
    "        print(f\"Saved model at epoch {epoch}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11467a17-3b8b-40b8-b5eb-d0de9fa0c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(range(1, N_EPOCHS + 1), train_av_loss_logger)\n",
    "_ = plt.plot(range(1, N_EPOCHS + 1), test_av_loss_logger)\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Av. Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
