{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86c2366-711e-4f59-8cfd-f6ea792c58b4",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Performance\n",
    "\n",
    "written by Isobel Mawby (i.mawby1@lancaster.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d788b-f09d-43b7-8576-743a514d4ea5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Imports\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f2d46-251a-4563-a0a9-b65f2e8dbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import Datasets\n",
    "import TrainingMetrics\n",
    "import Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34525e28-da6d-4c1b-803c-7b8ed766e29b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Set device\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e20fe-9f61-43e6-a035-d6e0dc123f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c93107-ea0e-4739-9aa0-da707378cfca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Config\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59eccf-7126-45ac-8cc4-1fec87f845f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_FRACTION = 0.75\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "N_EPOCHS = 1\n",
    "ALPHA = 2.0     # Loss scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69112f78-74f1-4d43-974e-73b3a424d084",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    File \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6809844-d828-419a-ba7d-e00963ec30d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelPath = sys.path[0] + '/files/SplitPointModel_UVW'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02c928-d335-4261-abbe-5f784af43a9c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Pull out things from file\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60c52b-71eb-4b19-82dc-f2803c9cf472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = Datasets.get_split_point_datasets(device, TRAINING_FRACTION)\n",
    "\n",
    "print('Input(train):', train_dataset.input.shape)\n",
    "print('Truth(train):', train_dataset.labels.shape)\n",
    "print('Contaminated(train):', train_dataset.is_contaminated.shape)\n",
    "print('')\n",
    "print('Input(test):', test_dataset.input.shape)\n",
    "print('Truth(test):', test_dataset.labels.shape)\n",
    "print('Contaminated(test):', test_dataset.is_contaminated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef45ab-1ea4-41e2-b103-5667680b7dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('n_background:', torch.count_nonzero(train_dataset.is_contaminated == 0).item())\n",
    "print('n_signal:', torch.count_nonzero(train_dataset.is_contaminated == 1).item())\n",
    "print('n_showers:', torch.count_nonzero(train_dataset.is_contaminated == 2).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f96b6-8c4e-4737-a014-03026946e0ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "     Do not train on showers, they confuse things\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802cb2f-9f57-413c-a9a3-44ce6e5c1fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_train = train_dataset.is_contaminated != 2\n",
    "mask_test = test_dataset.is_contaminated != 2\n",
    "\n",
    "train_dataset.input = train_dataset.input.unsqueeze(1)[mask_train]\n",
    "train_dataset.labels = train_dataset.labels.unsqueeze(1)[mask_train]\n",
    "train_dataset.is_contaminated = train_dataset.is_contaminated[mask_train].reshape(-1,1)\n",
    "\n",
    "test_dataset.input = test_dataset.input.unsqueeze(1)[mask_test]\n",
    "test_dataset.labels = test_dataset.labels.unsqueeze(1)[mask_test]\n",
    "test_dataset.is_contaminated = test_dataset.is_contaminated[mask_test].reshape(-1,1)\n",
    "\n",
    "print('Input(train):', train_dataset.input.shape)\n",
    "print('Truth(train):', train_dataset.labels.shape)\n",
    "print('Contaminated(train):', train_dataset.is_contaminated.shape)\n",
    "print('')\n",
    "print('Input(test):', test_dataset.input.shape)\n",
    "print('Truth(test):', test_dataset.labels.shape)\n",
    "print('Contaminated(test):', test_dataset.is_contaminated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434827d1-f9a6-4ca5-8a88-0eda0ef228bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0, generator=torch.Generator(device='cpu'))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=0, generator=torch.Generator(device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04773113-8412-4513-8cfb-43fddb13fef5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Class Weights\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26641bd5-fc01-443b-979f-00ac0a7dd977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nFalse = torch.count_nonzero(train_dataset.labels == 0).item()\n",
    "nTrue = torch.count_nonzero(train_dataset.labels == 1).item()\n",
    "maxValue = max(nTrue, nFalse)\n",
    "\n",
    "class_weights = torch.tensor([float(maxValue)/float(nFalse), float(maxValue)/float(nTrue)])\n",
    "\n",
    "print('nTrue:', nTrue)\n",
    "print('nFalse:', nFalse)\n",
    "print('weights:', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0603032-4354-410f-940f-e7d4e96ab22f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Setup training\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11c52b-148c-4ce3-87b0-f6320fe59cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get our model\n",
    "_, _, n_features = train_dataset.input.shape\n",
    "model = Models.ConvEncoderDecoder(device, num_features=n_features)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimiser = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimiser, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804afd8-a038-41e4-9e17-545a0572bcf9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Let's start training!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b774380-5961-43ef-9183-20195838be82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_function(prediction, truth, class_weights) :\n",
    "\n",
    "    b, l, _ = truth.shape\n",
    "    \n",
    "    # do not reduce, so we can handle masking\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    # Get loss_1\n",
    "    loss_1 = loss_func(prediction, truth.float())\n",
    "\n",
    "    # Apply weighting\n",
    "    loss_1[truth == 0] *= class_weights[0]\n",
    "    loss_1[truth == 1] *= class_weights[1]  \n",
    "    loss_1 = loss_1.squeeze(2)\n",
    "    loss_1 = (loss_1.sum(dim=1)/l)\n",
    "    \n",
    "    # Want to make sure that model doesn't produce an excessive number of split points\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "    n_true_splits =  truth.squeeze(2).sum(dim=1).float()\n",
    "    n_pred_splits =  prediction.squeeze(2).sum(dim=1).float()\n",
    "    diff = n_pred_splits - n_true_splits\n",
    "    relu = nn.ReLU()\n",
    "    loss_2 = relu(diff)\n",
    "    \n",
    "    loss_combined = loss_1 + (loss_2 * ALPHA)\n",
    "    loss_combined = loss_combined.sum() / b\n",
    "    \n",
    "    return loss_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07b462-1461-4ca2-a055-58022946eef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize progress bar for tracking epochs\n",
    "pbar = trange(0, N_EPOCHS, leave=True, desc=\"Epoch\")\n",
    "\n",
    "# Loggers for training and testing\n",
    "training_av_loss_logger = []\n",
    "test_av_loss_logger = []\n",
    "train_acc_logger = [[],[]]\n",
    "test_acc_logger = [[],[]]\n",
    "\n",
    "# Loop over each epoch\n",
    "for epoch in pbar:\n",
    "\n",
    "    # Metrics\n",
    "    train_loss_count = 0\n",
    "    test_loss_count = 0\n",
    "    train_acc_count = [0, 0]                                                                                                                                                                                \n",
    "    test_acc_count = [0, 0]                                                                                                                                                                                 \n",
    "    counts_train = [0, 0]                                                                                                                                                                                   \n",
    "    counts_test = [0, 0]   \n",
    "    \n",
    "    # Set the model to training mode\n",
    "    venusaurus.train()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Loop over each batch in the training dataset\n",
    "    for x_train, label_train, _ in tqdm(train_dataloader, desc=\"Training\", leave=True):\n",
    "        \n",
    "        # Make prediction\n",
    "        pred = model(x_train)\n",
    "        \n",
    "        # Compute the loss using cross-entropy loss\n",
    "        loss = loss_function(pred, label_train, class_weights)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "            \n",
    "    tt = time.time()\n",
    "    \n",
    "    # Update learning rate\n",
    "    before_lr = optimiser.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimiser.param_groups[0][\"lr\"]\n",
    "    print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))   \n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()  \n",
    "    \n",
    "    # Loop over each batch in the training dataset\n",
    "    for x_train, label_train, _ in tqdm(train_dataloader, desc=\"Validation(Train)\", leave=True):\n",
    "\n",
    "        # Make prediction\n",
    "        pred = model(x_train)\n",
    "        \n",
    "        # Compute the loss using cross-entropy loss\n",
    "        loss = loss_function(pred, label_train, class_weights)\n",
    "        train_loss_count += loss.item()\n",
    "\n",
    "        # Apply sigmoid for inference\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # Update training accuracy\n",
    "        label_train = label_train.reshape(-1)\n",
    "        pred = pred.reshape(-1)\n",
    "        pred = torch.round(pred)\n",
    "        \n",
    "        for i in range(2) :\n",
    "            train_acc_count[i] += torch.count_nonzero(torch.logical_and(torch.isclose(pred, label_train.float()), label_train == i))\n",
    "            counts_train[i] += torch.count_nonzero(label_train == i)      \n",
    "    \n",
    "    # Loop over each batch in the testing dataset\n",
    "    with torch.no_grad():\n",
    "        for x_test, label_test, _ in tqdm(test_dataloader, desc=\"Validation(Test)\", leave=True):    \n",
    "\n",
    "            # Make prediction\n",
    "            pred = model(x_test)\n",
    "\n",
    "            # Compute the loss using cross-entropy loss\n",
    "            loss = loss_function(pred, label_test, class_weights)  \n",
    "            test_loss_count += loss.item()\n",
    "\n",
    "            # Apply sigmoid for inference\n",
    "            pred = torch.sigmoid(pred)\n",
    "            \n",
    "            # Update training accuracy\n",
    "            label_test = label_test.reshape(-1)\n",
    "            pred = pred.reshape(-1)\n",
    "            pred = torch.round(pred)\n",
    "        \n",
    "            for i in range(2) :\n",
    "                test_acc_count[i] += torch.count_nonzero(torch.logical_and(torch.isclose(pred, label_test.float()), label_test == i))\n",
    "                counts_test[i] += torch.count_nonzero(label_test == i)                            \n",
    "            \n",
    "        tv = time.time()            \n",
    "        \n",
    "        # Prints for Epoch\n",
    "        train_av_loss_logger.append(train_loss_count / len(train_dataloader))\n",
    "        test_av_loss_logger.append(test_loss_count / len(test_dataloader))\n",
    "        print(f'Training Time: {tt-t0:.3f} s')\n",
    "        print(f'Validation Time: {tv-tt:.3f} s')\n",
    "        print('')\n",
    "        for i in range(2) :\n",
    "            train_acc = (train_acc_count[i] / counts_train[i]).item()\n",
    "            test_acc = (test_acc_count[i] / counts_test[i]).item()\n",
    "            \n",
    "            print(f'train/test accuracy for class {i}: {train_acc*100.0:.2f}%, {test_acc*100.0:.2f}%')\n",
    "            train_acc_logger[i].append(train_acc)\n",
    "            test_acc_logger[i].append(test_acc)             \n",
    "            \n",
    "        # Save for each epoch                                                                                                                                                            \n",
    "        model_cpu = model.to('cpu').eval()                                                                                                                                                \n",
    "        sm = torch.jit.script(model_cpu)                                                                                                                                                       \n",
    "        sm.save(f\"{modelPath}_alpha_\"+str(ALPHA)+\"_epoch_\" + str(epoch) +\".pt\")                                                                                                                                          \n",
    "        torch.save(model_cpu.state_dict(), f\"{modelPath}_alpha_\"+str(ALPHA)+\"_epoch_\" + str(epoch) +\".pkl\")                                                                                                         \n",
    "        model = model.to(device)                                                                                                                                                          \n",
    "        print(f\"Saved model at epoch {epoch} with test accuracy {test_acc:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11467a17-3b8b-40b8-b5eb-d0de9fa0c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(range(1, N_EPOCHS + 1), training_av_loss_logger)\n",
    "_ = plt.plot(range(1, N_EPOCHS + 1), test_av_loss_logger)\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Av. Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d31222-8329-405f-8440-638afec3a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2) :\n",
    "    _ = plt.figure(figsize=(10, 5))\n",
    "    _ = plt.plot(range(1, N_EPOCHS + 1), train_acc_logger[i])\n",
    "    _ = plt.plot(range(1, N_EPOCHS + 1), test_acc_logger[i])\n",
    "\n",
    "    _ = plt.legend([\"Train\", \"Test\"])\n",
    "    _ = plt.title(\"Training Vs Test Accuracy\")\n",
    "    _ = plt.xlabel(\"Epochs\")\n",
    "    _ = plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320716a-b914-4064-8859-9cc4c3e71624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chosen_epoch = 4\n",
    "\n",
    "modelPath_split_point = f\"{modelPath}_alpha_\"+str(ALPHA)+\"_epoch_\" + str(chosen_epoch) +\".pt\"\n",
    "#modelPath_split_point = \"/home/imawby/Venusaurus/files/SplitPosModel_U.pt\"\n",
    "\n",
    "print(modelPath_split_point)\n",
    "\n",
    "venusaurus_split_point = torch.jit.load(modelPath_split_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89fd520-8602-489e-8b65-2fe1368e0f11",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 18px;\">\n",
    "    Made some post-training performance plots\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345957a-719b-472a-a8c7-9556db5f228a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "venusaurus_split_point.eval()\n",
    "\n",
    "pred_final_train = []\n",
    "truth_train = []\n",
    "true_scores_train = []\n",
    "false_scores_train = []\n",
    "\n",
    "pred_final_val = []\n",
    "truth_val = []\n",
    "true_scores_val = []\n",
    "false_scores_val = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for x_train, label_train, _ in training_dataloader:         \n",
    "        \n",
    "        # Make prediction\n",
    "        pred = venusaurus_split_point(x_train.to('cpu')).to('cpu')\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # Move to correct device\n",
    "        label_train = label_train.to('cpu')\n",
    "        \n",
    "        # Sort out mask\n",
    "        label_train = label_train.reshape(-1)\n",
    "        pred = pred.reshape(-1)\n",
    "        \n",
    "        true_scores_train.extend(pred[label_train == 1].tolist())\n",
    "        false_scores_train.extend(pred[label_train == 0].tolist())\n",
    "        pred_final_train.extend(pred.tolist())\n",
    "        truth_train.extend(label_train.tolist()) \n",
    "    \n",
    "    for x_val, label_val, _ in validation_dataloader:          \n",
    "\n",
    "        # Make prediction\n",
    "        pred = venusaurus_split_point(x_val.to('cpu')).to('cpu')\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # Move to correct device\n",
    "        label_val = label_val.to('cpu')\n",
    "        \n",
    "        # Sort out mask\n",
    "        label_val = label_val.reshape(-1)\n",
    "        pred = pred.reshape(-1)\n",
    "        \n",
    "        true_scores_val.extend(pred[label_val == 1].tolist())\n",
    "        false_scores_val.extend(pred[label_val == 0].tolist())\n",
    "        pred_final_val.extend(pred.tolist())\n",
    "        truth_val.extend(label_val.tolist())        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad7c52-344e-442b-bbd4-6915496e931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_scores(true_scores_train, false_scores_train, true_scores_val, false_scores_val) :\n",
    "    \n",
    "    true_plotting_weights_train = 1.0 / float(true_scores_train.shape[0])\n",
    "    true_plotting_weights_train = torch.ones(true_scores_train.shape) * true_plotting_weights_train\n",
    "    false_plotting_weights_train = 1.0 / float(false_scores_train.shape[0])\n",
    "    false_plotting_weights_train = torch.ones(false_scores_train.shape) * false_plotting_weights_train\n",
    "    true_plotting_weights_val = 1.0 / float(true_scores_val.shape[0])\n",
    "    true_plotting_weights_val = torch.ones(true_scores_val.shape) * true_plotting_weights_val\n",
    "    false_plotting_weights_val = 1.0 / float(false_scores_val.shape[0])\n",
    "    false_plotting_weights_val = torch.ones(false_scores_val.shape) * false_plotting_weights_val    \n",
    "    \n",
    "    plt.hist(true_scores_train, bins=50, range=(0, 1.0), color='blue', label='signal_train', weights=true_plotting_weights_train, histtype='step', linestyle='solid')\n",
    "    plt.hist(false_scores_train, bins=50, range=(0, 1.0), color='red', label='background_train', weights=false_plotting_weights_train, histtype='step', linestyle='solid')\n",
    "    plt.hist(true_scores_val, bins=50, range=(0, 1.0), color='blue', label='signal_test', weights=true_plotting_weights_val, histtype='step', linestyle='dashed')\n",
    "    plt.hist(false_scores_val, bins=50, range=(0, 1.0), color='red', label='background_test', weights=false_plotting_weights_val, histtype='step', linestyle='dashed')    \n",
    "\n",
    "    \n",
    "    #plt.ylim(0, 0.8)\n",
    "    plt.yscale(\"log\")\n",
    "    \n",
    "    plt.xlabel('Classification Score')\n",
    "    #plt.ylabel('log(Proportion of Showers)')\n",
    "    plt.ylabel('Proportion of Showers')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()  \n",
    "    \n",
    "    \n",
    "    \n",
    "def draw_confusion_with_threshold(pred, labels, threshold):\n",
    "    \n",
    "    scores = pred.copy()\n",
    "    \n",
    "    n_classes = 2\n",
    "    \n",
    "    predicted_true_mask = scores > threshold\n",
    "    predicted_false_mask = np.logical_not(predicted_true_mask)\n",
    "    scores[predicted_true_mask] = 1\n",
    "    scores[predicted_false_mask] = 0\n",
    "\n",
    "    confMatrix = confusion_matrix(labels, scores)\n",
    "    \n",
    "    trueSums = np.sum(confMatrix, axis=1)\n",
    "    predSums = np.sum(confMatrix, axis=0)\n",
    "\n",
    "    trueNormalised = np.zeros(shape=(n_classes, n_classes))\n",
    "    predNormalised = np.zeros(shape=(n_classes, n_classes))\n",
    "\n",
    "    for trueIndex in range(n_classes) : \n",
    "        for predIndex in range(n_classes) :\n",
    "            nEntries = confMatrix[trueIndex][predIndex]\n",
    "            if trueSums[trueIndex] > 0 :\n",
    "                trueNormalised[trueIndex][predIndex] = float(nEntries) / float(trueSums[trueIndex])\n",
    "            if predSums[predIndex] > 0 :\n",
    "                predNormalised[trueIndex][predIndex] = float(nEntries) / float(predSums[predIndex])\n",
    "\n",
    "    displayTrueNorm = ConfusionMatrixDisplay(confusion_matrix=trueNormalised, display_labels=[\"False\", \"True\"])\n",
    "    displayTrueNorm.plot()\n",
    "\n",
    "    displayPredNorm = ConfusionMatrixDisplay(confusion_matrix=predNormalised, display_labels=[\"False\", \"True\"])\n",
    "    displayPredNorm.plot()\n",
    "\n",
    "\n",
    "\n",
    "#TrainingMetrics.plot_scores(np.array(true_scores_train), np.array(false_scores_train), np.array(true_scores_val), np.array(false_scores_val))\n",
    "plot_scores(np.array(true_scores_train), np.array(false_scores_train), np.array(true_scores_val), np.array(false_scores_val))\n",
    "#TrainingMetrics.draw_confusion_with_threshold(np.array(pred_final_train), np.array(truth_train), 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24382f-fc3c-4b8a-80ae-7b6f82ed75c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_confusion_with_threshold(np.array(pred_final_val), np.array(truth_val), 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054fad3-5bf2-4bd0-8d4c-c9301319da72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_final_val = np.array(pred_final_val)\n",
    "pred_final_val= np.round(pred_final_val)\n",
    "truth_val = np.array(truth_val)\n",
    "\n",
    "\n",
    "pos_pred_count = np.count_nonzero(np.logical_and(np.isclose(pred_final_val, truth_val), truth_val == 0))\n",
    "pos_true_count = np.count_nonzero(truth_val == 0)                            \n",
    "\n",
    "print(pos_pred_count / pos_true_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb26d5-2dd3-4fe0-8acc-69a956887315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad2499-0135-4363-a57b-c080885e0e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
