{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL vertex finding network training\n",
    "\n",
    "This notebook is designed to take the input and truth images generated by the <code>make_images.ipynb</code> notebook and train networks for vertex finding. This notebook generates models for each of the U, V and W views for each of the required passes.\n",
    "    \n",
    "Most of the cells below will not need any editing, but towards the bottom of the notebook you will find some additional markdown that describes what you may need to edit (essentially just some file locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload external libraries that change\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If a matplotlib plot command is issued, display the results in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imaging.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "\n",
    "def imagify(input, pred, truth, n=3, randomize=True, null_code=0):\n",
    "    \"\"\"Process input, prediction and mask data ready for display\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input tensor from a batch\n",
    "        predictions: Predictions tensor from a batch\n",
    "        truth: Truth tensor from a batch\n",
    "        n: The number of images to extract from the batch (default: 3)\n",
    "        randomize: Choose random images from the batch if True, choose the first n otherwise (default: True)\n",
    "        null_code: The null mask code (default: 0)\n",
    "            \n",
    "    Returns:\n",
    "        A tuple (if n == 1) or zip of the processed images ready for display.\n",
    "    \"\"\"\n",
    "    # Select the images to process\n",
    "    choices = np.random.choice(np.array(range(inputs.shape[0])), size=n) if randomize else np.array(range(n))\n",
    "    input_imgs = input[choices,0,...]\n",
    "    truth_imgs = truth[choices,...]\n",
    "\n",
    "    input_imgs = input_imgs.detach().cpu()\n",
    "    truth_imgs = truth_imgs.detach().cpu()\n",
    "    pred_imgs = pred[choices,...].detach().cpu()\n",
    "\n",
    "    # Remove non-hit regions\n",
    "    mask = truth_imgs == null_code\n",
    "    pred_imgs = np.argmax(pred_imgs, axis=1)\n",
    "    pred_imgs = np.ma.array(pred_imgs, mask = mask).filled(0)\n",
    "\n",
    "    return zip(input_imgs, truth_imgs, pred_imgs) if n > 1 else (input_imgs, truth_imgs, pred_imgs)\n",
    "\n",
    "\n",
    "def show_batch(epoch, batch, input, pred, truth, null_code=0, n=3, randomize=True):\n",
    "    \"\"\"Display the images for a given epoch and batch. Each row is a triplet of input, prediction and mask.\n",
    "\n",
    "    Args:\n",
    "        epoch: The current training epoch\n",
    "        batch: The current training batch\n",
    "        input: Input tensor from a batch\n",
    "        pred: Predictions tensor from a batch\n",
    "        truth: Truth tensor from a batch\n",
    "        n: The number of images to extract from the batch (default: 3)\n",
    "        randomize: Choose random images from the batch if True, choose the first n otherwise (default: True)\n",
    "        null_code: The null mask code (default: 0).\n",
    "    \"\"\"\n",
    "    global vertex_pass, view\n",
    "    ax = None\n",
    "    rows, cols, size = 1, 2, 9\n",
    "    cmap = \"magma\" #ListedColormap(['black', 'red'])\n",
    "    bounds = np.linspace(0, 19, 19)\n",
    "    norm = BoundaryNorm(boundaries=bounds, ncolors=19)\n",
    "    xtr = dict(cmap=cmap, norm=norm)\n",
    "    #norm = BoundaryNorm([0., 0.05, 1.], cmap.N)\n",
    "    #cmap = ListedColormap(['black', 'red', 'yellow'])\n",
    "    #norm = BoundaryNorm([0., 0.5, 1.5, 2.], cmap.N)\n",
    "    #xtr = dict(cmap=cmap, norm=norm, alpha=0.7)\n",
    "\n",
    "    images = imagify(input, pred, truth, n, randomize, null_code)\n",
    "\n",
    "    for i, imgs in enumerate(images):\n",
    "        raw, cls, net = imgs\n",
    "        pair = (cls, net)\n",
    "        fig, axs = plt.subplots(1, cols, figsize=(cols * size, size))\n",
    "        for img, ax in zip(pair, axs):\n",
    "            #ax.imshow(raw, cmap=\"gist_gray\")\n",
    "            ax.imshow(img, **xtr)\n",
    "            #ax.imshow(img, cmap=\"magma\")\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt, f\"outputs/images/pass{vertex_pass}/{view}/output_{epoch}_{batch}_{i}\")\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def save_figure(fig, name):\n",
    "    \"\"\"Output a matplotlib figure PNG, PDF and EPS formats.\n",
    "\n",
    "    Args:\n",
    "        fig (Figure): The matplotlib figure to save.\n",
    "        name (str): The output filename excluding extension.\n",
    "    \"\"\"\n",
    "    fig.savefig(name + \".png\", facecolor='w')\n",
    "    fig.savefig(name + \".pdf\")\n",
    "    #fig.savefig(name + \".eps\")\n",
    "\n",
    "\n",
    "def get_supported_formats():\n",
    "    \"\"\"Retrieve the supported image formats.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing strings of file format descriptions keyed by extension.\n",
    "    \"\"\"\n",
    "    return plt.gcf().canvas.get_supported_filetypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.py\n",
    "from functools import partial\n",
    "\n",
    "def flatten_model(module):\n",
    "    children = list(module.children())\n",
    "    if len(children) == 0:\n",
    "        return [module]\n",
    "    else:\n",
    "        flat_model = []\n",
    "        for child in children:\n",
    "            flat_model += flatten_model(child)\n",
    "        return flat_model\n",
    "\n",
    "\n",
    "class Hook:\n",
    "    def __init__(self, id, module, func):\n",
    "        self.id = id\n",
    "        self.name = module.__class__.__name__\n",
    "        self.hook = module.register_forward_hook(partial(func, self))\n",
    "    \n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.remove()\n",
    "\n",
    "\n",
    "def append_stats(hook, module, input, output):\n",
    "    if not module.training:\n",
    "        return\n",
    "    if not hasattr(hook, 'stats'):\n",
    "        hook.stats = ([],[],[])\n",
    "    means, stds, hists = hook.stats\n",
    "    means.append(output.data.mean())\n",
    "    stds.append(output.data.std())\n",
    "    hists.append(output.data.histc(40, -5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def maxpool():\n",
    "    \"\"\"Return a max pooling layer.\n",
    "    \n",
    "        The maxpooling layer has a kernel size of 2, a stride of 2 and no padding.\n",
    "\n",
    "        Returns:\n",
    "            The max pooling layer\n",
    "    \"\"\"\n",
    "    return nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "\n",
    "\n",
    "def dropout(prob):\n",
    "    \"\"\"Return a dropout layer.\n",
    "\n",
    "        Args:\n",
    "            prob: The probability that drop out will be applied.\n",
    "\n",
    "        Returns:\n",
    "            The dropout layer\n",
    "    \"\"\"\n",
    "    return nn.Dropout(prob)\n",
    "\n",
    "\n",
    "def reinit_layer(layer, leak = 0.0, use_kaiming_normal=True):\n",
    "    \"\"\"Reinitialises convolutional layer weights.\n",
    "    \n",
    "        The default Kaiming initialisation in PyTorch is not optimal, this method\n",
    "        reinitialises the layers using better parameters\n",
    "\n",
    "        Args:\n",
    "            seq_block: The layer to be reinitialised.\n",
    "            leak: The leakiness of ReLU (default: 0.0)\n",
    "            use_kaiming_normal: Use Kaiming normal if True, Kaiming uniform otherwise (default: True)\n",
    "    \"\"\"\n",
    "    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\n",
    "        if use_kaiming_normal:\n",
    "            nn.init.kaiming_normal_(layer.weight, a = leak)\n",
    "        else:\n",
    "            nn.init.kaiming_uniform_(layer.weight, a = leak)\n",
    "            layer.bias.data.zero_()\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"A convolution block\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sigmoid activation suitable for binary cross-entropy\n",
    "    def __init__(self, c_in, c_out, k_size = 3, k_pad = 1):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                c_in: The number of input channels\n",
    "                c_out: The number of output channels\n",
    "                k_size: The size of the convolution filter\n",
    "                k_pad: The amount of padding around the images\n",
    "        \"\"\"\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(c_in, c_out, kernel_size = k_size, padding = k_pad, stride = 1)\n",
    "        self.norm1 = nn.GroupNorm(8, c_out)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(c_out, c_out, kernel_size = k_size, padding = k_pad, stride = 1)\n",
    "        self.norm2 = nn.GroupNorm(8, c_out)\n",
    "        self.identity = nn.Conv2d(c_in, c_out, kernel_size = 1, padding = 0, stride = 1)\n",
    "        reinit_layer(self.conv1)\n",
    "        reinit_layer(self.conv2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "            Args:\n",
    "                x: The input to the layer\n",
    "                \n",
    "            Returns:\n",
    "                The output from the layer\n",
    "        \"\"\"\n",
    "        identity = self.identity(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        return self.relu(x + identity)\n",
    "\n",
    "\n",
    "class TransposeConvBlock(nn.Module):\n",
    "    \"\"\"A tranpose convolution block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, c_in, c_out, k_size = 3, k_pad = 1):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                c_in: The number of input channels\n",
    "                c_out: The number of output channels\n",
    "                k_size: The size of the convolution filter\n",
    "                k_pad: The amount of padding around the images\n",
    "        \"\"\"\n",
    "        super(TransposeConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(c_in, c_out, kernel_size = k_size, padding = k_pad, output_padding = 1, stride = 2),\n",
    "            nn.GroupNorm(8, c_out),\n",
    "            nn.ReLU(inplace=True))\n",
    "        reinit_layer(self.block[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "            Args:\n",
    "                x: The input to the layer\n",
    "                \n",
    "            Returns:\n",
    "                The output from the layer\n",
    "        \"\"\"\n",
    "        return self.block(x)\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    \"\"\"A sigmoid activation function that supports categorical cross-entropy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, out_range = None):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                out_range: A tuple covering the minimum and maximum values to map to\n",
    "        \"\"\"\n",
    "        super(Sigmoid, self).__init__()\n",
    "        if out_range is not None:\n",
    "            self.low, self.high = out_range\n",
    "            self.range = self.high - self.low\n",
    "        else:\n",
    "            self.low = None\n",
    "            self.high = None\n",
    "            self.range = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the sigmoid function.\n",
    "        \n",
    "            Rescales to the specified range if provided during construction\n",
    "        \n",
    "            Args:\n",
    "                x: The input to the layer\n",
    "                \n",
    "            Returns:\n",
    "                The (potentially scaled) sigmoid of the input\n",
    "        \"\"\"\n",
    "        if self.low is not None:\n",
    "            return torch.sigmoid(x) * (self.range) + self.low\n",
    "        else:\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"A U-Net for semantic segmentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, n_classes, depth = 4, n_filters = 16, drop_prob = 0.1, y_range = None):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                in_dim: The number of input channels\n",
    "                n_classes: The number of classes\n",
    "                depth: The number of convolution blocks in the downsampling and upsampling arms of the U (default: 4)\n",
    "                n_filters: The number of filters in the first layer (doubles for each downsample) (default: 16)\n",
    "                drop_prob: The dropout probability for each layer (default: 0.1)\n",
    "                y_range: The range of values (low, high) to map to in the output (default: None)\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        # Contracting Path\n",
    "        self.ds_conv_1 = ConvBlock(in_dim, n_filters)\n",
    "        self.ds_conv_2 = ConvBlock(n_filters, 2 * n_filters)\n",
    "        self.ds_conv_3 = ConvBlock(2 * n_filters, 4 * n_filters)\n",
    "        self.ds_conv_4 = ConvBlock(4 * n_filters, 8 * n_filters)\n",
    "\n",
    "        self.ds_maxpool_1 = maxpool()\n",
    "        self.ds_maxpool_2 = maxpool()\n",
    "        self.ds_maxpool_3 = maxpool()\n",
    "        self.ds_maxpool_4 = maxpool()\n",
    "        \n",
    "        self.ds_dropout_1 = dropout(drop_prob)\n",
    "        self.ds_dropout_2 = dropout(drop_prob)\n",
    "        self.ds_dropout_3 = dropout(drop_prob)\n",
    "        self.ds_dropout_4 = dropout(drop_prob)\n",
    "        \n",
    "        self.bridge = ConvBlock(8 * n_filters, 16 * n_filters)\n",
    "        \n",
    "        # Expansive Path\n",
    "        self.us_tconv_4 = TransposeConvBlock(16 * n_filters, 8 * n_filters)\n",
    "        self.us_tconv_3 = TransposeConvBlock(8 * n_filters, 4 * n_filters)\n",
    "        self.us_tconv_2 = TransposeConvBlock(4 * n_filters, 2 * n_filters)\n",
    "        self.us_tconv_1 = TransposeConvBlock(2 * n_filters, n_filters)\n",
    "\n",
    "        self.us_conv_4 = ConvBlock(16 * n_filters, 8 * n_filters)\n",
    "        self.us_conv_3 = ConvBlock(8 * n_filters, 4 * n_filters)\n",
    "        self.us_conv_2 = ConvBlock(4 * n_filters, 2 * n_filters)\n",
    "        self.us_conv_1 = ConvBlock(2 * n_filters, 1 * n_filters)\n",
    "\n",
    "        self.us_dropout_4 = dropout(drop_prob)\n",
    "        self.us_dropout_3 = dropout(drop_prob)\n",
    "        self.us_dropout_2 = dropout(drop_prob)\n",
    "        self.us_dropout_1 = dropout(drop_prob)\n",
    "\n",
    "        self.output = nn.Sequential(nn.Conv2d(n_filters, n_classes, 1), Sigmoid(y_range))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "            Args:\n",
    "                x: The input to the layer\n",
    "                \n",
    "            Returns:\n",
    "                The output from the layer\n",
    "        \"\"\"\n",
    "        res = x\n",
    "\n",
    "        # Downsample\n",
    "        res = self.ds_conv_1(res); conv_stack_1 = res.clone()\n",
    "        res = self.ds_maxpool_1(res)\n",
    "        res = self.ds_dropout_1(res)\n",
    "        \n",
    "        res = self.ds_conv_2(res); conv_stack_2 = res.clone()\n",
    "        res = self.ds_maxpool_2(res)\n",
    "        res = self.ds_dropout_2(res)\n",
    "        \n",
    "        res = self.ds_conv_3(res); conv_stack_3 = res.clone()\n",
    "        res = self.ds_maxpool_3(res)\n",
    "        res = self.ds_dropout_3(res)\n",
    "        \n",
    "        res = self.ds_conv_4(res); conv_stack_4 = res.clone()\n",
    "        res = self.ds_maxpool_4(res)\n",
    "        res = self.ds_dropout_4(res)\n",
    "        \n",
    "        # Bridge\n",
    "        res = self.bridge(res)\n",
    "        \n",
    "        # Upsample\n",
    "        res = self.us_tconv_4(res)\n",
    "        res = torch.cat([res, conv_stack_4], dim=1)\n",
    "        res = self.us_dropout_4(res)\n",
    "        res = self.us_conv_4(res)\n",
    "\n",
    "        res = self.us_tconv_3(res)\n",
    "        res = torch.cat([res, conv_stack_3], dim=1)\n",
    "        res = self.us_dropout_3(res)\n",
    "        res = self.us_conv_3(res)\n",
    "        \n",
    "        res = self.us_tconv_2(res)\n",
    "        res = torch.cat([res, conv_stack_2], dim=1)\n",
    "        res = self.us_dropout_2(res)\n",
    "        res = self.us_conv_2(res)\n",
    "        \n",
    "        res = self.us_tconv_1(res)\n",
    "        res = torch.cat([res, conv_stack_1], dim=1)\n",
    "        res = self.us_dropout_1(res)\n",
    "        res = self.us_conv_1(res)\n",
    "        \n",
    "        output = self.output(res)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# network.py\n",
    "\n",
    "# from model import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set the various seeds and flags to ensure deterministic performance\n",
    "    \n",
    "        Args:\n",
    "            seed: The random seed\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.deterministic = True   # Note, can impede performance\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def get_class_weights(stats):\n",
    "    \"\"\"Get the weights for each class\n",
    "    \n",
    "        Each class has a weight inversely proportional to the number of instances in the training set\n",
    "    \n",
    "        Args:\n",
    "            stats: The number of instances of each class\n",
    "        \n",
    "        Returns:\n",
    "            The weights for each class\n",
    "    \"\"\"\n",
    "    if np.any(stats == 0.):\n",
    "        print(\"Found a class that doesn't appear\")\n",
    "        idx = np.where(stats == 0.)\n",
    "        stats[idx] = 1\n",
    "        weights = 1. / stats\n",
    "        weights[idx] = 0\n",
    "    else:\n",
    "        weights = 1. / stats\n",
    "    return [weight / sum(weights) for weight in weights]\n",
    "\n",
    "\n",
    "def load_model_only(filename, num_classes, device):\n",
    "    \"\"\"Load a model\n",
    "\n",
    "        Args:\n",
    "            filename: The name of the file with the pretrained model parameters\n",
    "            num_classes: The number of classes available to predict\n",
    "            weights: The weights to apply to the classes\n",
    "            device: The device on which to run\n",
    "\n",
    "        Returns:\n",
    "            A tuple composed (in order) of the model, loss function, and optimiser\n",
    "    \"\"\"\n",
    "    model = UNet(1, n_classes = num_classes, depth = 4, n_filters = 16, y_range = (0, num_classes - 1))\n",
    "    model.load_state_dict(torch.load(filename, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(filename, num_classes, weights, device):\n",
    "    \"\"\"Load a model\n",
    "\n",
    "        Args:\n",
    "            filename: The name of the file with the pretrained model parameters\n",
    "            num_classes: The number of classes available to predict\n",
    "            weights: The weights to apply to the classes\n",
    "            device: The device on which to run\n",
    "\n",
    "        Returns:\n",
    "            A tuple composed (in order) of the model, loss function, and optimiser\n",
    "    \"\"\"\n",
    "    model = UNet(1, n_classes = num_classes, depth = 4, n_filters = 16, y_range = (0, num_classes - 1))\n",
    "    model.load_state_dict(torch.load(filename, map_location=device))\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(torch.as_tensor(weights, device=device, dtype=torch.float))\n",
    "    optim = opt.Adam(model.parameters())\n",
    "    return model, loss_fn, optim\n",
    "\n",
    "\n",
    "def save_model(model, input, filename):\n",
    "    \"\"\"Save the model\n",
    "    \n",
    "        The model is saved as both a pkl file and a TorchScript pt file, which can be loaded via\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            model.eval()\n",
    "        \n",
    "        Args:\n",
    "            model: The model to save\n",
    "            input: An example input to the model\n",
    "            filename: The output filename, without file extension\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), f\"{filename}.pkl\")\n",
    "\n",
    "    \n",
    "def accuracy(pred, truth, nearby=False):\n",
    "    \"\"\"Get the network accuracy\n",
    "    \n",
    "        Args:\n",
    "            pred: The network prediction\n",
    "            truth: The true class\n",
    "            nearby: Whether to consider adjacent classes acceptable\n",
    "        \n",
    "        Returns:\n",
    "            The accuracy\n",
    "    \"\"\"\n",
    "    target = truth.squeeze(1)\n",
    "    pred_cls = pred.argmax(dim=1)\n",
    "    mask = (target != 0)\n",
    "    if nearby:\n",
    "        result = abs(pred_cls[mask] - target[mask]) <= 1\n",
    "    else:\n",
    "        result = pred_cls[mask] == target[mask]\n",
    "    return result.float().mean()\n",
    "\n",
    "\n",
    "def create_model(num_classes, weights, device):\n",
    "    \"\"\"Create the model\n",
    "\n",
    "        Args:\n",
    "            num_classes: The number of classes available to predict\n",
    "            weights: The weights to apply to the classes\n",
    "            device: The device on which to run\n",
    "\n",
    "        Returns:\n",
    "            A tuple composed (in order) of the model, loss function, and optimiser\n",
    "    \"\"\"\n",
    "    model = UNet(1, n_classes = num_classes, depth = 4, n_filters = 16, y_range = (0, num_classes - 1))\n",
    "    loss_fn = nn.CrossEntropyLoss(torch.as_tensor(weights, device=device, dtype=torch.float))\n",
    "    optim = opt.Adam(model.parameters())\n",
    "    return model, loss_fn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Dataset suitable for segmentation tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, mask_dir, filenames, transform=False, device=torch.device('cuda:0')):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                image_dir: The directory containing the images\n",
    "                mask_dir: The directory containing the masks\n",
    "                filenames: The filanems for the images associate with this dataset\n",
    "                transform: Whether or not to transform the items (default: False).\n",
    "                device: The device on which tensors should be created (default: 'cuda:0')\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = filenames\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retrieve the number of samples in the dataset.\n",
    "        \n",
    "            Returns:\n",
    "                The number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.filenames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieve a sample from the dataset.\n",
    "        \n",
    "            Args:\n",
    "                idx: The index of the sample to be retrieved\n",
    "        \n",
    "            Returns:\n",
    "                The sample requested\n",
    "        \"\"\"\n",
    "        img_name = os.path.join(self.image_dir, self.filenames[idx])\n",
    "        with open(img_name, 'rb') as file:\n",
    "            image = np.load(file)['arr_0']\n",
    "        \n",
    "        mask_name = os.path.join(self.mask_dir, self.filenames[idx])\n",
    "        with open(mask_name, 'rb') as file:\n",
    "            mask = np.load(file)['arr_0']\n",
    "                \n",
    "        image = torch.as_tensor(np.expand_dims(image, axis=0), device=self.device, dtype=torch.float)\n",
    "        mask = torch.as_tensor(mask, device=self.device, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            should_hflip = True if torch.rand(1) > 0.5 else False\n",
    "            should_vflip = True if torch.rand(1) > 0.5 else False\n",
    "            should_transpose = True if torch.rand(1) > 0.5 else False\n",
    "            # need to check that these make sense in the context of pixel classification\n",
    "            if should_hflip:\n",
    "                image = tv.transforms.functional.hflip(image)\n",
    "                mask = tv.transforms.functional.hflip(mask)\n",
    "            if should_vflip:\n",
    "                image = tv.transforms.functional.vflip(image)\n",
    "                mask = tv.transforms.functional.vflip(mask)\n",
    "            if should_transpose:\n",
    "                image = image.transpose(1, 2)\n",
    "                mask = mask.transpose(1, 2)\n",
    "        \n",
    "        return (image, mask)\n",
    "\n",
    "\n",
    "class SegmentationBunch():\n",
    "    \"\"\"Associates batches of training, validation and testing datasets suitable\n",
    "        for segmentation tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, image_dir, mask_dir, batch_size, train_pct=None, valid_pct=0.1,\n",
    "                 test_pct=0.0, transform=False, device=torch.device('cuda:0')):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                root_dir: The top-level directory containing the images\n",
    "                image_dir: The relative directory containing the images\n",
    "                mask_dir: The relative directory containing the masks\n",
    "                batch_size: The batch size\n",
    "                valid_pct: The fraction of images to be used for validation (default: 0.1)\n",
    "                test_pct: The fraction of images to be used for testing (default: 0.0)\n",
    "                transform: Whether or not to transform the items (default: False)\n",
    "                device: The device on which tensors should be created (default: 'cuda:0')\n",
    "        \"\"\"\n",
    "        assert((valid_pct + test_pct) < 1.)\n",
    "        image_dir = os.path.join(root_dir, image_dir)\n",
    "        mask_dir = os.path.join(root_dir, mask_dir)\n",
    "        image_filenames = np.array(next(os.walk(image_dir))[2])\n",
    "        print(image_filenames)\n",
    "        n_files = len(image_filenames)\n",
    "        valid_size = int(n_files * valid_pct)\n",
    "        train_size = n_files - valid_size if train_pct is None else int(n_files * train_pct)\n",
    "        \n",
    "        sample = np.random.permutation(n_files)\n",
    "        train_sample = sample[valid_size:] if not train_size else \\\n",
    "            sample[valid_size:valid_size + train_size]\n",
    "        valid_sample = sample[:valid_size]\n",
    "                \n",
    "        train_ds = SegmentationDataset(image_dir, mask_dir, image_filenames[train_sample], transform, device)\n",
    "        self.train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=0)\n",
    "        \n",
    "        valid_ds = SegmentationDataset(image_dir, mask_dir, image_filenames[valid_sample], None, device)\n",
    "        self.valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=0)\n",
    "\n",
    "\n",
    "    def count_classes(self, num_classes):\n",
    "        \"\"\"Count the number of instances of each class in the training set\n",
    "        \n",
    "            Args:\n",
    "                num_classes: The number of classes in the training set\n",
    "                \n",
    "            Returns:\n",
    "                A list of the number of instances of each class\n",
    "        \"\"\"\n",
    "        count = np.zeros(num_classes)\n",
    "        for batch in self.train_dl:\n",
    "            _, truth = batch\n",
    "            unique, counts = torch.unique(truth, return_counts=True)\n",
    "            unique = [ u.item() for u in unique ]\n",
    "            counts = [ c.item() for c in counts ]\n",
    "            this_dict = dict(zip(unique, counts))\n",
    "            for key in this_dict:\n",
    "                count[key] += this_dict[key]\n",
    "        return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run network training here\n",
    "\n",
    "The key parameters that will need editing are the view and the pass to be trained. Each view/pass combination has its own network. Views are specified using the standard U, V, W nomenclature (and is consistent with the file naming conventions from previous steps), while the pass is either pass 1 or pass 2.\n",
    "\n",
    "The respective variables can be set in the cell below via <code>view</code> and <code>vertex_pass</code>.\n",
    "\n",
    "If you edited the <code>thresholds</code> variable at the <code>make_images</code> stage, you may need to update the <code>NUM_CLASSES</code> variable to reflect any change in the number of thresholds. Note that this value should be equal to the length of the <code>thresholds</code> variable, despite this variable specifying bin edges, because one extra class is required to represent the null case where a pixel has no hits in it.\n",
    "\n",
    "<code>batch_size</code> can, of course, be varied according to the available resources of your GPU, but as a semantic segmentation network you'll need a lot of memory on your GPU to increase this beyond the current default of 32.\n",
    "\n",
    "The <code>image_path</code> variable should contain the path to the images generated by the <code>make_images</code> notebook (i.e. <code>global_path</code>), and will expect to find the <code>Hits</code> and <code>Truth</code> folders within that path).\n",
    "\n",
    "Note that the cells below will count the class representation in the training set, determine how to weight them and print this out. It's worth taking a look at this output to ensure all classes are represented, as training will fail if they are not.\n",
    "\n",
    "You will want to set the number of epochs, <code>n_epochs</code> to train for. This is not easy to determine a priori, but 20 is a reasonable starting point (plots of the loss function and accuracy are produced to help you determine when the network has effectively trained).\n",
    "\n",
    "<code>model_name</code> acts as a prefix for saving the model. The state of the model is saved after every epoch, with a suffix indicating the epoch number.\n",
    "\n",
    "Once you are happy with the variable values you can run all of the cells in this section in order (having run all of the cells above), with the final cell in this section actually performing the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is important for GPU running, otherwise some weights end up on the CPU\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "view = \"C\"\n",
    "vertex_pass = 2\n",
    "the_seed = 42\n",
    "gpu = torch.device('cuda:0')\n",
    "batch_size=32\n",
    "NUM_CLASSES = 20   # NULL = 0, various distance bands 1-19\n",
    "image_path = f\"Pass{vertex_pass}/Images{view}\"\n",
    "n_epochs = 20\n",
    "model_name = \"pass1\"\n",
    "\n",
    "for subdir in [\"models\", \"stats\", \"images\"]:\n",
    "    dir = f\"outputs/{subdir}/pass{vertex_pass}/{view}\"\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "#from data import *\n",
    "#from network import *\n",
    "\n",
    "set_seed(the_seed)\n",
    "bunch = SegmentationBunch(image_path, \"Hits\", \"Truth\", batch_size=batch_size, valid_pct = 0.25, device=gpu)\n",
    "train_stats = bunch.count_classes(NUM_CLASSES)\n",
    "weights = get_class_weights(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = torch.zeros(n_epochs * len(bunch.train_dl), device=gpu)\n",
    "val_losses = torch.zeros(n_epochs, device=gpu)\n",
    "batch_losses = torch.zeros(len(bunch.valid_dl), device=gpu)\n",
    "\n",
    "train_accs = torch.zeros(n_epochs * len(bunch.train_dl), device=gpu)\n",
    "val_accs = torch.zeros(n_epochs, device=gpu)\n",
    "batch_accs = torch.zeros(len(bunch.valid_dl), device=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard model creation\n",
    "model, loss_fn, optim = create_model(NUM_CLASSES, weights, gpu)\n",
    "\n",
    "i = 0\n",
    "start = 0\n",
    "finish = n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "set_seed(the_seed)\n",
    "for e in tqdm(range(start, finish), desc=\"Training\"):\n",
    "    model = model.train()\n",
    "    n_batches = len(bunch.train_dl)\n",
    "    for b, batch in enumerate(bunch.train_dl):\n",
    "        x, y = batch\n",
    "        pred = model.forward(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        train_losses[i] = loss.item()\n",
    "        train_accs[i] = accuracy(pred, y, nearby=False)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #scheduler.step()\n",
    "        optim.zero_grad()\n",
    "        i += 1\n",
    "        if b == (n_batches - 1):\n",
    "            save_model(model, x, f\"outputs/models/pass{vertex_pass}/{view}/{model_name}_{e}\")\n",
    "\n",
    "    # Validate\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for b, batch in enumerate(bunch.valid_dl):\n",
    "            x, y = batch\n",
    "            pred = model.forward(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            batch_losses[b] = loss.item()\n",
    "            batch_accs[b] = accuracy(pred, y, nearby=False)\n",
    "        val_losses[e] = torch.mean(batch_losses)\n",
    "        val_accs[e] = torch.mean(batch_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess network performance\n",
    "\n",
    "Having trained a network, you can look at its performance - this should be run immediately after the network has finished training. The cells below should not require any editing.\n",
    "\n",
    "The first cell runs over a single batch from the validation set and produuces images allowing you to compare the truth (left image) to the network classification (right image), though it is worth noting that the <code>show_batch</code> function produces all images in the same folder and does not uniquely identify the view or pass, so if you want to retain them, you'll want to move them between runs.\n",
    "\n",
    "The next three cells produce plots showing the evolution of the network across epochs. Ideally you want to see a plateauing of the loss and accuracy to establish a well trained model, with no evidence that the training and validation performance are diverging (you can always select a model from an earlier epoch before divergence if the network appears to be overfitting - or get more training samples if the network is not adequately trained).\n",
    "\n",
    "The final cell in this section simply saves the evolution history of the network to allow easy plot regenertion if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(the_seed)\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    for b, batch in enumerate(bunch.valid_dl):\n",
    "        x, y = batch\n",
    "        pred = model.forward(x)\n",
    "        show_batch(finish, b, x, pred, y, n=32, randomize=False)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['axes.titlesize'] = 32\n",
    "mpl.rcParams['axes.labelsize'] = 32\n",
    "mpl.rcParams['xtick.labelsize'] = 22\n",
    "mpl.rcParams['ytick.labelsize'] = 22\n",
    "mpl.rcParams['legend.fontsize'] = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 15))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('metric')\n",
    "\n",
    "tl = torch.mean(train_losses.reshape([n_epochs, -1]), axis=1).detach().cpu()\n",
    "vl = val_losses.detach().cpu()\n",
    "plt.plot(tl, label=\"training loss\")\n",
    "plt.plot(vl, label=\"validation loss\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "fig.savefig(f\"outputs/stats/pass{vertex_pass}/{view}/stats_loss_{vertex_pass}_{view}.pdf\", dpi=200)\n",
    "fig.savefig(f\"outputs/stats/pass{vertex_pass}/{view}/stats_loss_{vertex_pass}_{view}.png\", dpi=200, facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 15))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('metric')\n",
    "\n",
    "ta = torch.mean(train_accs.reshape([n_epochs, -1]), axis=1).detach().cpu()\n",
    "va = val_accs.detach().cpu()\n",
    "plt.plot(ta, label=\"training accuracy\")\n",
    "plt.plot(va, label=\"validation accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "fig.savefig(f\"outputs/stats/pass{vertex_pass}/{view}/stats_acc_{vertex_pass}_{view}.pdf\", dpi=200)\n",
    "fig.savefig(f\"outputs/stats/pass{vertex_pass}/{view}/stats_acc_{vertex_pass}_{view}.png\", dpi=200, facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/stats/pass{vertex_pass}/{view}/train_loss_{vertex_pass}_{view}_20.npy', 'wb') as f:\n",
    "    np.save(f, tl)\n",
    "with open(f'outputs/stats/pass{vertex_pass}/{view}/val_loss_{vertex_pass}_{view}_20.npy', 'wb') as f:\n",
    "    np.save(f, vl)\n",
    "with open(f'outputs/stats/pass{vertex_pass}/{view}/train_accs_{vertex_pass}_{view}_20.npy', 'wb') as f:\n",
    "    np.save(f, ta)\n",
    "with open(f'outputs/stats/pass{vertex_pass}/{view}/val_accs_{vertex_pass}_{view}_20.npy', 'wb') as f:\n",
    "    np.save(f, va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a TorchScript network\n",
    "\n",
    "The network was trained on a GPU, but ultimately runs on a CPU in a C++ context. This means that the network must be converted to TorchScript format. This can be performed using the cell below and can be run at any time - it need not be run immediately after training the network, because it only requires access to a saved model state.\n",
    "\n",
    "The only parameters requiring editing here are the location of the input file; which is the save model from the chosen training epoch (so some combination of the <code>moidel_name</code> and epoch with a <code>.pkl</code> extension), the <code>output_filename</code>, which should have a <code>.pt</code> extension, and also the number of classes <code>NUM_CLASSES</code>, which should, of course, match the previouslyt specified value.\n",
    "\n",
    "The resultant <code>.pt</code> files are what will ultimately be loaded by Pandora for network inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is important for ensuring all tensors exist on the same device\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "filename = f\"outputs/models/pass{vertex_pass}/{view}/pass1_19.pkl\"\n",
    "output_filename = f\"PandoraUnet_Vertex_{vertex_pass}_{view}.pt\"\n",
    "the_seed = 42\n",
    "device = torch.device('cpu')\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "set_seed(the_seed)\n",
    "\n",
    "model = load_model_only(filename, NUM_CLASSES, device)\n",
    "\n",
    "sm = torch.jit.script(model)\n",
    "sm.save(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is important for GPU running, otherwise some weights end up on the CPU\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "view = \"C\"\n",
    "vertex_pass = 2\n",
    "the_seed = 42\n",
    "gpu = torch.device('cuda:0')\n",
    "batch_size=1\n",
    "NUM_CLASSES = 20   # NULL = 0, various distance bands 1-19\n",
    "image_path = f\"Pass{vertex_pass}/Images{view}\"\n",
    "\n",
    "for subdir in [\"models\", \"stats\", \"images\"]:\n",
    "    dir = f\"outputs/{subdir}/pass{vertex_pass}/{view}\"\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "set_seed(the_seed)\n",
    "bunch = SegmentationBunch(image_path, \"Hits\", \"Truth\", batch_size=batch_size, valid_pct = 0.25, device=gpu)\n",
    "\n",
    "filename = f\"outputs/models/pass{vertex_pass}/{view}/pass1_19.pkl\"\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "set_seed(the_seed)\n",
    "\n",
    "model = load_model_only(filename, NUM_CLASSES, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "binning = np.linspace(0, 20, 21, dtype=int)\n",
    "\n",
    "model = model.to(gpu)\n",
    "confusion = np.zeros((20,20))\n",
    "for img, cls in bunch.valid_dl:\n",
    "    img = img.to(gpu)\n",
    "    output = model(img)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    \n",
    "    cls_detached = cls.cpu().numpy().flatten()\n",
    "    preds_detached = preds.cpu().numpy().flatten()\n",
    "    \n",
    "    H, *_ = stats.binned_statistic_2d(preds_detached, cls_detached, None,\n",
    "                                      bins=[binning, binning], statistic='count')\n",
    "    confusion += H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = confusion.copy()\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.xlabel('true class')\n",
    "plt.ylabel('fraction')\n",
    "plt.step(list(np.arange(1, 20)), np.sum(temporary[1:], axis=1) / np.sum(temporary[1:]), where=\"mid\")\n",
    "\n",
    "save_figure(fig, f\"outputs/stats/pass{vertex_pass}/{view}/true_class_dist_{vertex_pass}_{view}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(confusion, axis=1).repeat(20).reshape((20,20))\n",
    "confusion /= sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Class Accuracy\")\n",
    "for t in range(confusion.shape[0]):\n",
    "    print(f\"{t:2}: {100*(confusion[t,t] / confusion[t].sum()):.1f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion[0,:] = 0\n",
    "confusion[:,0] = 0\n",
    "\n",
    "sums = np.sum(confusion, axis=1).repeat(20).reshape((20,20))\n",
    "sums[0,:] = 1\n",
    "confusion /= sums\n",
    "\n",
    "print(f\"--- Class Accuracy\")\n",
    "for t in range(confusion.shape[0]):\n",
    "    print(f\"{t:2}: {100*confusion[t,t]:.1f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_xlabel('truth')\n",
    "ax.set_ylabel('network')\n",
    "im = ax.imshow(confusion, cmap='Blues', vmin=0, vmax=1, origin='lower')\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(im, cax=cax)\n",
    "tick_marks = np.arange(20)\n",
    "ax.set_xticks(tick_marks, tick_marks)\n",
    "ax.set_yticks(tick_marks, tick_marks)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, f\"outputs/stats/pass{vertex_pass}/{view}/confusion_{vertex_pass}_{view}\")\n",
    "\n",
    "for t in range(20):\n",
    "    for n in range(20):\n",
    "        print(f\"{confusion[t, n]:.2f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "271px",
    "left": "14.0001px",
    "right": "20px",
    "top": "233px",
    "width": "479px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
