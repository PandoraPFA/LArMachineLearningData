{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL vertex input image generation\n",
    "\n",
    "This notebook is designed to take CSV files generated by the <code>PrepareTrainingSample</code> function of the <code>DlVertexingAlgorithm</code>. This algorithm generates CSV files for each of the U. V and W views and the code below will run over each of those files.\n",
    "    \n",
    "Most of the cells below will not need any editing, but at the very bottom of the notebook you will find some additional markdown that describes what you may need to edit (essentially just some file locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload external libraries that change\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If a matplotlib plot command is issued, display the results in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wire_pitches(detector):\n",
    "    if detector == \"dunefd_hd\":\n",
    "        return { \"U\": 0.46669998765, \"V\": 0.46669998765, \"W\": 0.479000002146 }\n",
    "    elif detector == \"sbnd\":\n",
    "        return { \"I\": 0.300000011921, \"C\": 0.300000011921 }\n",
    "        #return { \"U\": 0.300000011921, \"V\": 0.300000011921, \"W\": 0.300000011921 }\n",
    "    else:\n",
    "        raise ValueError(f\"Detector '{detector}' not known\")\n",
    "\n",
    "def get_drift_step(detector):\n",
    "    if detector == \"dunefd_hd\":\n",
    "        return 0.5\n",
    "    elif detector == \"sbnd\":\n",
    "        return 0.3\n",
    "    else:\n",
    "        raise ValueError(f\"Detector '{detector}' not known\")\n",
    "\n",
    "def make_input_histogram(x, z, adc, vertex, x_bounds, z_bounds, image_size, view):\n",
    "    global thresholds\n",
    "    image_height, image_width = image_size\n",
    "    x_min, x_max = x_bounds\n",
    "    z_min, z_max = z_bounds\n",
    "    \n",
    "    # Update the span if image is too small\n",
    "    r_span = np.sqrt((x_max - x_min)**2 + (z_max - z_min)**2)\n",
    "    \n",
    "    x_bins = np.linspace(x_min - 0.5 * drift_step, x_max + 0.5 * drift_step, image_width + 1)\n",
    "    z_bins = np.linspace(z_min - 0.5 * wire_pitch[view], z_max + 0.5 * wire_pitch[view], image_height + 1)\n",
    "    \n",
    "    phx = np.digitize(x, x_bins) - 1\n",
    "    phz = np.digitize(z, z_bins) - 1\n",
    "    pha = np.digitize(adc, z_bins) - 1\n",
    "        \n",
    "    pvx = np.digitize(vertex[0], x_bins) - 1\n",
    "    pvz = np.digitize(vertex[1], z_bins) - 1\n",
    "\n",
    "    input_histogram, _, _ = np.histogram2d(z, x, bins=[z_bins, x_bins], weights=adc)\n",
    "    input_histogram = input_histogram.astype(float)\n",
    "\n",
    "    # ATTN: Need to set the dtype here or truth will be of type 8-bit uint before scaling, which will result in\n",
    "    # incorrect distances for dr > 255\n",
    "    truth_histogram = np.zeros_like(input_histogram, dtype=float)\n",
    "    \n",
    "    # Handle case where vertex is outside of the hit bounding box. Note, it may still be possible for the correction\n",
    "    # to move the vertex outside the bounding box in the opposite direction (unlikely, but think about it)\n",
    "    # other way to do this is to bound the image including the vertex location\n",
    "    if pvx < 0: # underflow\n",
    "        pvx = -(np.digitize(x_bins[0] + (x_bins[0] - vertex[0]), x_bins) - 1)\n",
    "    elif pvx >= (len(x_bins) - 1): # overflow\n",
    "        pvx = np.digitize(x_bins[-1] - (vertex[0] - x_bins[-1]), x_bins) - 1\n",
    "    if pvz < 0: # underflow\n",
    "        pvz = -(np.digitize(z_bins[0] + (z_bins[0] - vertex[1]), z_bins) - 1)\n",
    "    elif pvz >= (len(z_bins) - 1): # overflow\n",
    "        pvz = np.digitize(z_bins[-1] - (vertex[1] - z_bins[-1]), z_bins) - 1\n",
    "\n",
    "    dr = np.sqrt((phx - pvx)**2 + (phz - pvz)**2)\n",
    "    class_histogram = np.zeros_like(truth_histogram)\n",
    "    for i in range(len(phx)):\n",
    "        truth_histogram[phz[i], phx[i]] = dr[i]\n",
    "    truth_min, truth_max = np.min(truth_histogram), np.max(truth_histogram)\n",
    "    if truth_max > truth_min:\n",
    "        truth_histogram = (truth_histogram - truth_min) / np.ceil(np.sqrt(2*(image_height - 1)**2))\n",
    "        for i in range(len(phx)):\n",
    "            cls = np.digitize(truth_histogram[phz[i], phx[i]], thresholds)\n",
    "            class_histogram[phz[i], phx[i]] = cls if cls < len(thresholds) else len(thresholds) - 1\n",
    "    else:\n",
    "        class_histogram = np.zeros_like(input_histogram)\n",
    "    class_histogram = class_histogram.astype('uint8')\n",
    "\n",
    "    return input_histogram, class_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def display(hits_x, hits_z, vrt_x, vrt_z):\n",
    "    \"\"\"Displays an image of the hits and vertices.\n",
    "    \n",
    "        Args:\n",
    "            hits_x: list of x coordinates for hits\n",
    "            hits_z: list of z coordinates for hits\n",
    "            vrt_x: list of x coordinates for vertices\n",
    "            vrt_z: list of z coordinates for vertices\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.scatter(hits_x, hits_z, c='black', s=20, alpha=1.0, label=\"Hits\")\n",
    "    plt.scatter(vrt_x, vrt_z, c='red', s=50, alpha=1.0, label=\"Vertices\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "def process_file(input_file, output_folder, view, image_size = (128, 128)):\n",
    "    \"\"\"Generate the training/validation set images for events.\n",
    "    \n",
    "        The input CSV file has the format:\n",
    "        Date/Time,N Vertices,M Hits,N*{vertex x, vertex y},M*{hit x, hit y},EOL\n",
    "        \n",
    "        The first vertex in the list is always the primary vertex\n",
    "    \n",
    "        Args:\n",
    "            input_file: a CSV file containing event information\n",
    "            output_folder: the top-level folder for output images\n",
    "            image_size: the output image size as a tuple (height, width)\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        num_events = len(f.readlines())\n",
    "    with open(input_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(tqdm(reader, desc=\"Processing views\", miniters=100, total=num_events)):\n",
    "            data = row[1:-1]\n",
    "            process_event(data, output_folder, f\"{i}\", view, image_size)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def process_event(data, output_folder, event, view, image_size):\n",
    "    \"\"\"Generate the training/validation set images for a single event.\n",
    "    \n",
    "        The input data has the format:\n",
    "        N Vertices,M Hits,N*{vertex x, vertex y},M*{hit x, hit z, adc}\n",
    "        \n",
    "        The first vertex in the list is always the primary vertex\n",
    "        \n",
    "        Images are output to <output_folder>/Hits and <output_folder>/Truth\n",
    "    \n",
    "        Args:\n",
    "            data: the set of vertices and hits for the event\n",
    "            output_folder: the top-level folder for output images\n",
    "            event: the event number\n",
    "            image_size: the output image size as a tuple (height, width)\n",
    "    \"\"\"\n",
    "    nv_coords = 2\n",
    "    nh_coords = 3\n",
    "    nuance = int(data.pop(0))\n",
    "    n_vertices = int(data.pop(0))\n",
    "    \n",
    "    v_start, v_finish = 0, nv_coords * n_vertices\n",
    "    vx, vz = np.array(data[v_start:v_finish:2], dtype=float), np.array(data[v_start + 1:v_finish:2], dtype=float)\n",
    "\n",
    "    b_start = v_finish\n",
    "    x_min, x_max = float(data[b_start]), float(data[b_start + 1])\n",
    "    z_min, z_max = float(data[b_start + 2]), float(data[b_start + 3])\n",
    "    \n",
    "    n_hits = int(data[b_start + 4])\n",
    "    h_start, h_finish = b_start + 5, b_start + 5 + nh_coords * n_hits\n",
    "    length = len(data[h_start:])\n",
    "    \n",
    "    if length != (n_hits * nh_coords):\n",
    "        print('Missing information in input file')\n",
    "        print(n_hits, length)\n",
    "        return\n",
    "    \n",
    "    hx = np.array(data[h_start:h_finish:nh_coords], dtype=float)\n",
    "    hz = np.array(data[h_start + 1:h_finish:nh_coords], dtype=float)\n",
    "    hadc = np.array(data[h_start + 2:h_finish:nh_coords], dtype=float)\n",
    "\n",
    "    if hx.size == 0 or hz.size == 0 or hadc.size == 0:\n",
    "        return\n",
    "    \n",
    "    input_histogram, truth_histogram = make_input_histogram(hx, hz, hadc, (vx[0], vz[0]),\n",
    "                                                            (x_min, x_max), (z_min, z_max), image_size, view)\n",
    "    \n",
    "    hits_output_folder = os.path.join(output_folder, \"Hits\")\n",
    "    hits_filename = os.path.join(hits_output_folder, f\"Image_{event}.npz\")\n",
    "    with open(hits_filename, 'wb') as file:\n",
    "        np.savez_compressed(file, input_histogram)\n",
    "   \n",
    "    truth_output_folder = os.path.join(output_folder, \"Truth\")\n",
    "    truth_filename = os.path.join(truth_output_folder, f\"Image_{event}.npz\")\n",
    "    with open(truth_filename, 'wb') as file:\n",
    "        np.savez_compressed(file, truth_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit below this point\n",
    "\n",
    "The details that might change between different contexts are the input and output file locations, the class thresholds and potentially the number of passes.\n",
    "\n",
    "The input files are specified by the <code>file_prefix</code> variable - the <code>PrepareTrainingSample</code> function automatically tags the files with their respective views, so you should omit the view and file type from the specification.\n",
    "\n",
    "The output location is specified by <code>global_path</code>, within which <code>Hit</code> and <code>Truth</code> folders will be created to store the input and target output images for training.\n",
    "\n",
    "The thresholds are specified by the <code>thresholds</code> variable, a global variable referenced by <code>make_input_histograms</code>. It is critical that the values here match those specified in the Pandora XML specification for the vertexing algorithm.\n",
    "\n",
    "In general, you will want to implement a two pass approach to networking, as this will likely greatly enhance vertex resolution, but if you may only want one pass due to higher pass dependence on earlier passes for CSV generation, you can just alter the <code>vertex_pass</code> loop list to run over the selected pass. There is no explicit dependency between the passes in this notebook.\n",
    "\n",
    "Once you're happy with these values, you can just run the entire notebook from top to bottom and, after some time, you'll have a set of input/truth images that can be used to train the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholds(detector):\n",
    "    if detector == \"dunefd_hd\":\n",
    "        return [0., 0.00275, 0.00825, 0.01925, 0.03575, 0.05775, 0.08525, \\\n",
    "              0.12375, 0.15125, 0.20625, 0.26125, 0.31625, 0.37125, 0.42625, \\\n",
    "              0.50875, 0.59125, 0.67375, 0.75625, 0.85, 1.0]\n",
    "    elif detector == \"sbnd\":\n",
    "        return [0., 0.00275, 0.00825, 0.01925, 0.03575, 0.05775, 0.08525, \\\n",
    "              0.12375, 0.16375, 0.20625, 0.26125, 0.31625, 0.37125, 0.42625, \\\n",
    "              0.50875, 0.59125, 0.67375, 0.75625, 0.85, 1.0]\n",
    "    else:\n",
    "        raise ValueError(f\"Detector '{detector}' not known\")\n",
    "\n",
    "wire_pitch = get_wire_pitches(\"sbnd\")\n",
    "drift_step = get_drift_step(\"sbnd\")\n",
    "thresholds = get_thresholds(\"sbnd\")\n",
    "\n",
    "for vertex_pass in [1]:\n",
    "    file_prefix = f\"csv/Pass{vertex_pass}_CaloHitList\"\n",
    "    image_size = (256, 256) if vertex_pass == 1 else (128, 128)\n",
    "    for view in [\"C\", \"I\"]:\n",
    "        global_path = os.path.join(f\"Pass{vertex_pass}\", f\"Images{view}\")\n",
    "        for path in [os.path.join(global_path, \"Hits\"), os.path.join(global_path, \"Truth\")]:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        process_file(file_prefix + view + \".csv\", global_path, view, image_size = image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"Pass{vertex_pass}/ImagesC/Truth/Image_0.npz\", 'rb') as file:\n",
    "    check = np.load(file)['arr_0']\n",
    "plt.imshow(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
